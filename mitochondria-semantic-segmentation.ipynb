{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-12-15T07:13:12.123956Z",
     "iopub.status.busy": "2021-12-15T07:13:12.123555Z",
     "iopub.status.idle": "2021-12-15T07:13:12.129035Z",
     "shell.execute_reply": "2021-12-15T07:13:12.127473Z",
     "shell.execute_reply.started": "2021-12-15T07:13:12.123877Z"
    }
   },
   "outputs": [],
   "source": [
    "# # This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# # For example, here's several helpful packages to load\n",
    "\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# # Input data files are available in the read-only \"../input/\" directory\n",
    "# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "# #         print(os.path.join(dirname, filename))\n",
    "\n",
    "# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-14T10:22:38.323023Z",
     "iopub.status.busy": "2021-12-14T10:22:38.322734Z",
     "iopub.status.idle": "2021-12-14T10:22:41.605145Z",
     "shell.execute_reply": "2021-12-14T10:22:41.604181Z",
     "shell.execute_reply.started": "2021-12-14T10:22:38.322973Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "import tqdm\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from skimage import io, transform,color\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms as T\n",
    "# from albumentations.pytorch import ToTensor\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, utils, models\n",
    "from torch import nn\n",
    "from functools import wraps\n",
    "# from torchvision import models\n",
    "\n",
    "from albumentations import (HorizontalFlip, ShiftScaleRotate, Normalize, Resize, Compose, GaussNoise)\n",
    "import cv2\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "import zipfile\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T10:43:51.446777Z",
     "iopub.status.busy": "2021-12-07T10:43:51.446176Z",
     "iopub.status.idle": "2021-12-07T10:43:51.46299Z",
     "shell.execute_reply": "2021-12-07T10:43:51.46215Z",
     "shell.execute_reply.started": "2021-12-07T10:43:51.446741Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Set the path for lucchi and kasthuri dataset.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "train_img_path_luchi = \"/kaggle/input/segmentation-dataset/lucchi_pp/Lucchi++/Train_In/\"\n",
    "train_mask_path_luchi = \"/kaggle/input/segmentation-dataset/lucchi_pp/Lucchi++/Train_Out/\"\n",
    "\n",
    "test_img_path_luchi = \"/kaggle/input/segmentation-dataset/lucchi_pp/Lucchi++/Test_In/\"\n",
    "test_mask_path_luchi = \"/kaggle/input/segmentation-dataset/lucchi_pp/Lucchi++/Test_Out/\"\n",
    "\n",
    "\n",
    "train_img_path_kasthuri = \"/kaggle/input/segmentation-dataset/kasthuri_pp/Kasthuri++/Train_In/\"\n",
    "train_mask_path_kasthuri = \"/kaggle/input/segmentation-dataset/kasthuri_pp/Kasthuri++/Train_Out/\"\n",
    "\n",
    "test_img_path_kasthuri = \"/kaggle/input/segmentation-dataset/kasthuri_pp/Kasthuri++/Test_In/\"\n",
    "test_mask_path_kasthuri = \"/kaggle/input/segmentation-dataset/kasthuri_pp/Kasthuri++/Test_Out/\"\n",
    "\n",
    "\n",
    "glas_path = '../input/glas-dataset/Warwick QU Dataset (Released 2016_07_08)/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T10:44:09.520544Z",
     "iopub.status.busy": "2021-12-07T10:44:09.519964Z",
     "iopub.status.idle": "2021-12-07T10:44:09.531742Z",
     "shell.execute_reply": "2021-12-07T10:44:09.530442Z",
     "shell.execute_reply.started": "2021-12-07T10:44:09.520504Z"
    }
   },
   "outputs": [],
   "source": [
    "# def mask_convert(mask):\n",
    "#     mask = mask.clone().cpu().detach().numpy()\n",
    "#     mask = mask.transpose((1,2,0))\n",
    "#     std = np.array((0.5))\n",
    "#     mean = np.array((0.5))\n",
    "#     mask  = std * mask + mean\n",
    "#     mask = mask.clip(0,1)\n",
    "#     mask = np.squeeze(mask)\n",
    "#     return mask\n",
    "\n",
    "# # converting tensor to image\n",
    "# def image_convert(image):\n",
    "#     image = image.clone().cpu().numpy()\n",
    "#     image = image.transpose((1,2,0))\n",
    "#     std = np.array((0.5,0.5,0.5))\n",
    "#     mean = np.array((0.5,0.5,0.5))\n",
    "#     image  = std * image + mean\n",
    "#     image = image.clip(0,1)\n",
    "#     image = (image * 255).astype(np.uint8)\n",
    "#     return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T10:44:40.55989Z",
     "iopub.status.busy": "2021-12-07T10:44:40.559284Z",
     "iopub.status.idle": "2021-12-07T10:44:40.573989Z",
     "shell.execute_reply": "2021-12-07T10:44:40.573202Z",
     "shell.execute_reply.started": "2021-12-07T10:44:40.559855Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function that returns a series of augmentations.\n",
    "\"\"\"\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "\n",
    "def get_valid_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Normalize(mean=(0),std=(1))\n",
    "        ],\n",
    "        p=1.0)\n",
    "\n",
    "\n",
    "def light_training_transforms():\n",
    "    return A.Compose([\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.Transpose(),\n",
    "                A.VerticalFlip(),\n",
    "                A.HorizontalFlip(),\n",
    "                A.RandomRotate90(),\n",
    "                A.NoOp(),\n",
    "            ], p=1.0),\n",
    "    ])\n",
    "\n",
    "\n",
    "def medium_training_transforms():\n",
    "    return A.Compose([\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.Transpose(),\n",
    "                A.VerticalFlip(),\n",
    "                A.HorizontalFlip(),\n",
    "                A.RandomRotate90(),\n",
    "                A.NoOp()\n",
    "            ], p=1.0),\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.CoarseDropout(max_holes=16, max_height=16, max_width=16),\n",
    "                A.NoOp()\n",
    "            ], p=1.0),\n",
    "    ])\n",
    "\n",
    "# 0.5517, std = 0.1187\n",
    "\n",
    "\n",
    "def heavy_training_transforms():\n",
    "    return A.Compose([\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.Transpose(),\n",
    "                A.VerticalFlip(),\n",
    "                A.HorizontalFlip(),\n",
    "                A.RandomRotate90(),\n",
    "                A.NoOp(),\n",
    "            ], p=1.0),\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.CLAHE(),\n",
    "#                 A.RGBShift(),\n",
    "                A.RandomBrightnessContrast(),\n",
    "                A.RandomGamma(),\n",
    "#                 A.HueSaturationValue(),\n",
    "                A.NoOp(),\n",
    "            ], p=0.15),\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.ElasticTransform(),\n",
    "                A.GridDistortion(),\n",
    "                A.OpticalDistortion(),\n",
    "                A.NoOp(),\n",
    "                A.ShiftScaleRotate(),\n",
    "            ], p=1.0),\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.GaussNoise(),\n",
    "                A.GaussianBlur(),\n",
    "                A.NoOp(),\n",
    "            ], p=0.15),\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.CoarseDropout(max_holes=16, max_height=16, max_width=16),\n",
    "                A.NoOp(),\n",
    "            ], p=1.0),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T10:44:40.575655Z",
     "iopub.status.busy": "2021-12-07T10:44:40.575236Z",
     "iopub.status.idle": "2021-12-07T10:44:40.588069Z",
     "shell.execute_reply": "2021-12-07T10:44:40.587231Z",
     "shell.execute_reply.started": "2021-12-07T10:44:40.575588Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Funciton to pair image and its corresponding mask in a list.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def preProcessImg_luchi_kasthuri(train_path,test_path,data):\n",
    "  regex = \"^0+(?!$)\"\n",
    "\n",
    "  train_mask = []\n",
    "  train_image = []\n",
    "\n",
    "  test_img = []\n",
    "  test_mask = []\n",
    "\n",
    "  if data == 'luchi':\n",
    "    for file in os.listdir(train_path):\n",
    "      files = file\n",
    "      if files.endswith('png'):\n",
    "        files = files.split('mask')[1]\n",
    "        files = re.sub(regex, \"\", files)\n",
    "        if files == '.png':\n",
    "          files = '0.png'\n",
    "        train_image.append(file)\n",
    "        train_mask.append(files)\n",
    "\n",
    "    for file in os.listdir(test_path):\n",
    "      files = file\n",
    "      if files.endswith('png'):\n",
    "        files = files.split(\"mask\")[1]\n",
    "        files = re.sub(regex, \"\", files)\n",
    "        if files == '.png':\n",
    "          files = '0.png'\n",
    "        test_img.append(file)\n",
    "        test_mask.append(files)\n",
    "  else:\n",
    "    for file in os.listdir(train_path):\n",
    "      if file.endswith('png'):\n",
    "        train_image.append(file)\n",
    "        train_mask.append(file)\n",
    "\n",
    "    for file in os.listdir(test_path):\n",
    "      if file.endswith('png'):\n",
    "        test_img.append(file)\n",
    "        test_mask.append(file)\n",
    "\n",
    "  return train_image,train_mask,test_img,test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T10:44:40.589866Z",
     "iopub.status.busy": "2021-12-07T10:44:40.589369Z",
     "iopub.status.idle": "2021-12-07T10:44:40.623144Z",
     "shell.execute_reply": "2021-12-07T10:44:40.622271Z",
     "shell.execute_reply.started": "2021-12-07T10:44:40.589827Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The dataset class that returns the (image, mask) in its tensor form.\n",
    "\n",
    "Dataset_Preparation takes the lucchi training images. It takes the dataframe(containing the path to image and its mask) and the augmentation function as input.\n",
    "Dataset_Preparation_test takes the lucchi test images. It takes the dataframe(containing the path to image and its mask).\n",
    "Dataset_Preparation_kasthuri takes the kasthuri training images. It takes the dataframe(containing the path to image and its mask) and the augmentation function as input.\n",
    "Dataset_Preparation_kasthuri_test takes the kasthuri test images. It takes the dataframe(containing the path to image and its mask).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Dataset_Preparation(Dataset):\n",
    "  def __init__(self,df,transform=None,Normalize_transform = None):\n",
    "    self.df = df\n",
    "    self.transforms = transform\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.df)\n",
    "\n",
    "  def __getitem__(self,idx):\n",
    "    train_path = train_img_path_luchi + self.df['image'][idx]\n",
    "    mask_path = train_mask_path_luchi + self.df['mask'][idx]\n",
    "\n",
    "    X  = cv2.imread(train_path, cv2.IMREAD_GRAYSCALE)\n",
    "    X = np.expand_dims(X,axis=-1)\n",
    "    \n",
    "    Y = cv2.imread(mask_path,cv2.IMREAD_GRAYSCALE)\n",
    "    Y = np.expand_dims(Y,axis=-1)\n",
    "    \n",
    "    mask = np.zeros((Y.shape[0],Y.shape[1],1))\n",
    "    mask = np.maximum(mask,Y)\n",
    "    mask = mask.astype(\"float32\")\n",
    "    \n",
    "    if transform:\n",
    "        transformed = self.transforms(image=X, mask = mask)\n",
    "        img = transformed['image']\n",
    "        mask = transformed['mask']\n",
    "    \n",
    "\n",
    "    \n",
    "    img = cv2.resize(img,(224,224),interpolation=cv2.INTER_NEAREST)\n",
    "    mask = cv2.resize(mask,(224,224),interpolation=cv2.INTER_NEAREST)    \n",
    "    mask = mask/255\n",
    "    img = img/255\n",
    "#     img=img-0.5517\n",
    "#     img=img/0.1187\n",
    "    mask = np.expand_dims(mask,axis=-1).transpose((2,0,1))\n",
    "    img = np.expand_dims(img,axis=0)\n",
    "        \n",
    "    img = torch.from_numpy(img)\n",
    "    img.unsqueeze(1)\n",
    "    mask = torch.from_numpy(mask)\n",
    "\n",
    "    return (img,mask)\n",
    "\n",
    "\n",
    "class Dataset_Preparation_test(Dataset):\n",
    "  def __init__(self,df,transform = None):\n",
    "    self.df = df\n",
    "    self.transforms = transform\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.df)\n",
    "\n",
    "  def __getitem__(self,idx):\n",
    "    img_path = test_img_path_luchi + self.df['image'][idx]\n",
    "    mask_path = test_mask_path_luchi + self.df['mask'][idx]\n",
    "\n",
    "    X = cv2.imread(img_path,cv2.IMREAD_GRAYSCALE)\n",
    "    X = np.expand_dims(X,axis=-1)\n",
    "\n",
    "    Y = cv2.imread(mask_path,cv2.IMREAD_GRAYSCALE)\n",
    "    Y = np.expand_dims(Y,axis=-1)\n",
    "\n",
    "    mask = np.zeros((Y.shape[0],Y.shape[1],1))\n",
    "    mask = np.maximum(mask,Y)\n",
    "    mask = mask.astype(\"float32\")\n",
    "    \n",
    "#     if transform:\n",
    "#         transformed = self.transforms(image=X, mask = mask)\n",
    "#         img = transformed['image']\n",
    "#         mask = transformed['mask']\n",
    "    \n",
    "    img = cv2.resize(X,(224,224),interpolation=cv2.INTER_NEAREST)\n",
    "    mask = cv2.resize(mask,(224,224),interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    mask = mask/255\n",
    "    img = img/255\n",
    "    mask = np.expand_dims(mask,axis=-1).transpose((2,0,1))\n",
    "    img = np.expand_dims(img,axis=0)\n",
    "\n",
    "    img = torch.from_numpy(img)\n",
    "    img.unsqueeze(0)\n",
    "    mask = torch.from_numpy(mask)\n",
    "\n",
    "    return (img,mask)\n",
    "\n",
    "\n",
    "class Dataset_Preparation_kasthuri(Dataset):\n",
    "  def __init__(self,df,transform=None,Normalize_transform = None):\n",
    "    self.df = df\n",
    "    self.transforms = transform\n",
    "    self.Normalize = Normalize_transform\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.df)\n",
    "\n",
    "  def __getitem__(self,idx):\n",
    "    img_path = train_img_path_kasthuri + self.df['image'][idx]\n",
    "    mask_path = train_mask_path_kasthuri + self.df['mask'][idx]\n",
    "\n",
    "    X = cv2.imread(img_path,cv2.IMREAD_GRAYSCALE)\n",
    "    X = np.expand_dims(X,axis=-1)\n",
    "    Y = cv2.imread(mask_path,cv2.IMREAD_GRAYSCALE)\n",
    "    Y = np.where(Y==2,0,Y)\n",
    "    Y = np.expand_dims(Y,axis=-1)\n",
    "\n",
    "    mask = np.zeros((Y.shape[0],Y.shape[1],1))\n",
    "    mask = np.maximum(mask,Y)\n",
    "    mask = mask.astype('float32')\n",
    "\n",
    "#     img,mask = self.transforms(X,mask)\n",
    "    if transform:\n",
    "        transformed = self.transforms(image=X, mask = mask)\n",
    "        img = transformed['image']\n",
    "        mask = transformed['mask']\n",
    "\n",
    "    img = cv2.resize(img,(224,224),interpolation = cv2.INTER_NEAREST)\n",
    "    mask = cv2.resize(mask,(224,224),interpolation = cv2.INTER_NEAREST)\n",
    "\n",
    "    mask = mask/255\n",
    "    img = img/255\n",
    "\n",
    "    mask = np.expand_dims(mask,axis=-1).transpose((2,0,1))\n",
    "    img = np.expand_dims(img,axis=0)\n",
    "    # print(img.shape)\n",
    "    \n",
    "    \n",
    "    img = torch.from_numpy(img)\n",
    "    img.unsqueeze(0)\n",
    "    mask = torch.from_numpy(mask)\n",
    "\n",
    "    return (img,mask)\n",
    "\n",
    "\n",
    "class Dataset_Preparation_kasthuri_test(Dataset):\n",
    "  def __init__(self,df,transform=None):\n",
    "    self.df = df\n",
    "    self.transforms = transform\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.df)\n",
    "\n",
    "  def __getitem__(self,idx):\n",
    "    img_path = test_img_path_kasthuri + self.df['image'][idx]\n",
    "    mask_path = test_mask_path_kasthuri + self.df['mask'][idx]\n",
    "\n",
    "#     X = io.imread(img_path)\n",
    "    X = cv2.imread(img_path,cv2.IMREAD_GRAYSCALE)\n",
    "    X = np.expand_dims(X,axis=-1)\n",
    "\n",
    "    Y = cv2.imread(mask_path,cv2.IMREAD_GRAYSCALE)\n",
    "    Y = np.where(Y==2,0,Y)\n",
    "#     Y = color.rgb2gray(Y)\n",
    "    Y = np.expand_dims(Y,axis=-1)\n",
    "\n",
    "    mask = np.zeros((Y.shape[0],Y.shape[1],1))\n",
    "    mask = np.maximum(mask,Y)\n",
    "    mask = mask.astype('float32')\n",
    "\n",
    "    # img,mask = self.transforms(X,mask)\n",
    "    \n",
    "#     if transform:\n",
    "#         transformed = self.transforms(image=X, mask = mask)\n",
    "#         img = transformed['image']\n",
    "#         mask = transformed['mask']\n",
    "    \n",
    "    img = cv2.resize(X,(224,224),interpolation = cv2.INTER_NEAREST)\n",
    "    mask = cv2.resize(mask,(224,224),interpolation = cv2.INTER_NEAREST)\n",
    "\n",
    "    mask = mask/255\n",
    "    img = img/255\n",
    "\n",
    "    mask = np.expand_dims(mask,axis=-1).transpose((2,0,1))\n",
    "    img = np.expand_dims(img,axis=0)\n",
    "    # print(img.shape)\n",
    "\n",
    "    img = torch.from_numpy(img)\n",
    "    img.unsqueeze(0)\n",
    "    mask = torch.from_numpy(mask)\n",
    "\n",
    "    return (img,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T10:44:40.646217Z",
     "iopub.status.busy": "2021-12-07T10:44:40.645585Z",
     "iopub.status.idle": "2021-12-07T10:44:40.821349Z",
     "shell.execute_reply": "2021-12-07T10:44:40.82053Z",
     "shell.execute_reply.started": "2021-12-07T10:44:40.64618Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "returns img and its mask as list which is later converted into a dataframe.\n",
    "\"\"\"\n",
    "\n",
    "train_img_luchi, train_mask_luchi, test_img_luchi,test_mask_luchi = preProcessImg_luchi_kasthuri(train_img_path_luchi,test_img_path_luchi,data='luchi')\n",
    "train_img_kasthuri, train_mask_kasthuri, test_img_kasthuri,test_mask_kasthuri = preProcessImg_luchi_kasthuri(train_img_path_kasthuri,test_img_path_kasthuri,data = 'kasthuri')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Create dataframe from the output of the \"preProcessImg_luchi_kasthuri\" function.\n",
    "\"\"\"\n",
    "df_train_luchi = pd.DataFrame({\"image\":train_img_luchi,\"mask\":train_mask_luchi})\n",
    "df_test_luchi = pd.DataFrame({\"image\":test_img_luchi,\"mask\":test_mask_luchi})\n",
    "\n",
    "df_train_kasthuri = pd.DataFrame({\"image\":train_img_kasthuri, \"mask\":train_mask_kasthuri})\n",
    "df_test_kasthuri = pd.DataFrame({\"image\":test_img_kasthuri, \"mask\":test_mask_kasthuri})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "create a (image,mask) tensor pair object of Dataset class.\n",
    "\"\"\"\n",
    "data_train_luchi = Dataset_Preparation(df_train_luchi,heavy_training_transforms())\n",
    "data_test_luchi = Dataset_Preparation_test(df_test_luchi)\n",
    "\n",
    "data_train_kasthuri = Dataset_Preparation_kasthuri(df_train_kasthuri,heavy_training_transforms())\n",
    "data_test_kasthuri = Dataset_Preparation_kasthuri_test(df_test_kasthuri)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Divide the object of Dataset class into train and val \n",
    "\"\"\"\n",
    "train_split_luchi = int(data_train_luchi.__len__()*0.8)\n",
    "test_split_luchi  = int(data_train_luchi.__len__()-train_split_luchi)\n",
    "\n",
    "train_split_kasthuri = int(data_train_kasthuri.__len__()*0.8)\n",
    "test_split_kasthuri  = int(data_train_kasthuri.__len__()-train_split_kasthuri)\n",
    "\n",
    "trainset_luchi, valset_luchi = random_split(data_train_luchi, [train_split_luchi,test_split_luchi])\n",
    "trainset_kasthuri,valset_kasthuri = random_split(data_train_kasthuri,[train_split_kasthuri,test_split_kasthuri])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Creates a DataLoader object for train, val and test set \n",
    "\"\"\"\n",
    "train_loader_segmentation_luchi = torch.utils.data.DataLoader(dataset=trainset_luchi, batch_size=4,shuffle=True)\n",
    "val_loader_segmentation_luchi = torch.utils.data.DataLoader(dataset=valset_luchi, batch_size=4,shuffle=True)\n",
    "test_loader_segmentation_luchi = torch.utils.data.DataLoader(dataset=data_test_luchi,batch_size=4,shuffle=True)\n",
    "\n",
    "train_loader_segmentation_kasthuri = torch.utils.data.DataLoader(dataset=trainset_kasthuri, batch_size=4,shuffle=True)\n",
    "val_loader_segmentation_kasthuri = torch.utils.data.DataLoader(dataset=valset_kasthuri, batch_size=4,shuffle=True)\n",
    "test_loader_segmentation_kasthuri = torch.utils.data.DataLoader(dataset=data_test_kasthuri,batch_size=4,shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T10:44:41.145117Z",
     "iopub.status.busy": "2021-12-07T10:44:41.144539Z",
     "iopub.status.idle": "2021-12-07T10:45:41.829238Z",
     "shell.execute_reply": "2021-12-07T10:45:41.828343Z",
     "shell.execute_reply.started": "2021-12-07T10:44:41.145062Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"show one instance of image and its mask.\"\"\"\n",
    "\n",
    "for idx, (img,mask) in enumerate(train_loader_segmentation_kasthuri):\n",
    "    \n",
    "    if idx==1:\n",
    "#         print(\"Hello\")\n",
    "#         print(img.shape)\n",
    "#         print(mask.shape)\n",
    "#         print(np.unique(mask[0].detach().cpu().numpy().flatten()))\n",
    "#         print(np.unique(img[0].detach().cpu().numpy().flatten()))\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.imshow(np.asarray(img[0].detach().cpu()).squeeze())\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.imshow(np.asarray(mask[0].detach().cpu()).squeeze())\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T10:45:41.8343Z",
     "iopub.status.busy": "2021-12-07T10:45:41.83401Z",
     "iopub.status.idle": "2021-12-07T10:45:41.841887Z",
     "shell.execute_reply": "2021-12-07T10:45:41.840742Z",
     "shell.execute_reply.started": "2021-12-07T10:45:41.834272Z"
    }
   },
   "outputs": [],
   "source": [
    "# some utility functions\n",
    "def mask_convert(mask):\n",
    "    mask = mask.clone().cpu().detach().numpy()\n",
    "    mask = mask.transpose((1,2,0))\n",
    "    std = np.array((0.5))\n",
    "    mean = np.array((0.5))\n",
    "    mask  = std * mask + mean\n",
    "    mask = mask.clip(0,1)\n",
    "    mask = np.squeeze(mask)\n",
    "    return mask\n",
    "\n",
    "# converting tensor to image\n",
    "def image_convert(image):\n",
    "    image = image.clone().cpu().numpy()\n",
    "    image = image.transpose((1,2,0))\n",
    "    std = np.array((0.5,0.5,0.5))\n",
    "    mean = np.array((0.5,0.5,0.5))\n",
    "    image  = std * image + mean\n",
    "    image = image.clip(0,1)\n",
    "    image = (image * 255).astype(np.uint8)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T10:45:41.845037Z",
     "iopub.status.busy": "2021-12-07T10:45:41.844608Z",
     "iopub.status.idle": "2021-12-07T10:45:41.85627Z",
     "shell.execute_reply": "2021-12-07T10:45:41.855394Z",
     "shell.execute_reply.started": "2021-12-07T10:45:41.845002Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_img(no_):\n",
    "    iter_ = iter(train_loader)\n",
    "    images,masks = next(iter_)\n",
    "    images = images.to(device)\n",
    "    masks = masks.to(device)\n",
    "    plt.figure(figsize=(10,6))\n",
    "    for idx in range(0,no_):\n",
    "        \n",
    "        image = image_convert(images[idx])\n",
    "        plt.subplot(2,no_,idx+1)\n",
    "        plt.title('image')\n",
    "        plt.imshow(image)\n",
    "    for idx in range(0,no_):\n",
    "        mask = mask_convert(masks[idx])\n",
    "        plt.subplot(2,no_,idx+no_+1)\n",
    "        plt.title('mask')\n",
    "        plt.imshow(mask,cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T10:45:41.858202Z",
     "iopub.status.busy": "2021-12-07T10:45:41.85778Z",
     "iopub.status.idle": "2021-12-07T10:45:41.891972Z",
     "shell.execute_reply": "2021-12-07T10:45:41.891131Z",
     "shell.execute_reply.started": "2021-12-07T10:45:41.858168Z"
    }
   },
   "outputs": [],
   "source": [
    "#stand alone self-attention module\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "class AttentionConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, groups=1, bias=False):\n",
    "        super(AttentionConv, self).__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.groups = groups\n",
    "\n",
    "        assert self.out_channels % self.groups == 0, \"out_channels should be divided by groups. (example: out_channels: 40, groups: 4)\"\n",
    "\n",
    "        self.rel_h = nn.Parameter(torch.randn(out_channels // 2, 1, 1, kernel_size, 1), requires_grad=True)\n",
    "        self.rel_w = nn.Parameter(torch.randn(out_channels // 2, 1, 1, 1, kernel_size), requires_grad=True)\n",
    "\n",
    "        self.key_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n",
    "        self.query_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n",
    "        self.value_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, channels, height, width = x.size()\n",
    "\n",
    "        padded_x = F.pad(x, [self.padding, self.padding, self.padding, self.padding])\n",
    "        q_out = self.query_conv(x)\n",
    "        k_out = self.key_conv(padded_x)\n",
    "        v_out = self.value_conv(padded_x)\n",
    "\n",
    "        k_out = k_out.unfold(2, self.kernel_size, self.stride).unfold(3, self.kernel_size, self.stride)\n",
    "        v_out = v_out.unfold(2, self.kernel_size, self.stride).unfold(3, self.kernel_size, self.stride)\n",
    "\n",
    "        k_out_h, k_out_w = k_out.split(self.out_channels // 2, dim=1)\n",
    "        k_out = torch.cat((k_out_h + self.rel_h, k_out_w + self.rel_w), dim=1)\n",
    "\n",
    "        k_out = k_out.contiguous().view(batch, self.groups, self.out_channels // self.groups, height, width, -1)\n",
    "        v_out = v_out.contiguous().view(batch, self.groups, self.out_channels // self.groups, height, width, -1)\n",
    "\n",
    "        q_out = q_out.view(batch, self.groups, self.out_channels // self.groups, height, width, 1)\n",
    "\n",
    "        out = q_out * k_out\n",
    "        out = F.softmax(out, dim=-1)\n",
    "        out = torch.einsum('bnchwk,bnchwk -> bnchw', out, v_out).view(batch, -1, height, width)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.kaiming_normal_(self.key_conv.weight, mode='fan_out', nonlinearity='relu')\n",
    "        init.kaiming_normal_(self.value_conv.weight, mode='fan_out', nonlinearity='relu')\n",
    "        init.kaiming_normal_(self.query_conv.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "        init.normal_(self.rel_h, 0, 1)\n",
    "        init.normal_(self.rel_w, 0, 1)\n",
    "\n",
    "\n",
    "class AttentionStem(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, groups=1, m=4, bias=False):\n",
    "        super(AttentionStem, self).__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.groups = groups\n",
    "        self.m = m\n",
    "\n",
    "        assert self.out_channels % self.groups == 0, \"out_channels should be divided by groups. (example: out_channels: 40, groups: 4)\"\n",
    "\n",
    "        self.emb_a = nn.Parameter(torch.randn(out_channels // groups, kernel_size), requires_grad=True)\n",
    "        self.emb_b = nn.Parameter(torch.randn(out_channels // groups, kernel_size), requires_grad=True)\n",
    "        self.emb_mix = nn.Parameter(torch.randn(m, out_channels // groups), requires_grad=True)\n",
    "\n",
    "        self.key_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n",
    "        self.query_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n",
    "        self.value_conv = nn.ModuleList([nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias) for _ in range(m)])\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, channels, height, width = x.size()\n",
    "\n",
    "        padded_x = F.pad(x, [self.padding, self.padding, self.padding, self.padding])\n",
    "\n",
    "        q_out = self.query_conv(x)\n",
    "        k_out = self.key_conv(padded_x)\n",
    "        v_out = torch.stack([self.value_conv[_](padded_x) for _ in range(self.m)], dim=0)\n",
    "\n",
    "        k_out = k_out.unfold(2, self.kernel_size, self.stride).unfold(3, self.kernel_size, self.stride)\n",
    "        v_out = v_out.unfold(3, self.kernel_size, self.stride).unfold(4, self.kernel_size, self.stride)\n",
    "\n",
    "        k_out = k_out[:, :, :height, :width, :, :]\n",
    "        v_out = v_out[:, :, :, :height, :width, :, :]\n",
    "\n",
    "        emb_logit_a = torch.einsum('mc,ca->ma', self.emb_mix, self.emb_a)\n",
    "        emb_logit_b = torch.einsum('mc,cb->mb', self.emb_mix, self.emb_b)\n",
    "        emb = emb_logit_a.unsqueeze(2) + emb_logit_b.unsqueeze(1)\n",
    "        emb = F.softmax(emb.view(self.m, -1), dim=0).view(self.m, 1, 1, 1, 1, self.kernel_size, self.kernel_size)\n",
    "\n",
    "        v_out = emb * v_out\n",
    "\n",
    "        k_out = k_out.contiguous().view(batch, self.groups, self.out_channels // self.groups, height, width, -1)\n",
    "        v_out = v_out.contiguous().view(self.m, batch, self.groups, self.out_channels // self.groups, height, width, -1)\n",
    "        v_out = torch.sum(v_out, dim=0).view(batch, self.groups, self.out_channels // self.groups, height, width, -1)\n",
    "\n",
    "        q_out = q_out.view(batch, self.groups, self.out_channels // self.groups, height, width, 1)\n",
    "\n",
    "        out = q_out * k_out\n",
    "        out = F.softmax(out, dim=-1)\n",
    "        out = torch.einsum('bnchwk,bnchwk->bnchw', out, v_out).view(batch, -1, height, width)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.kaiming_normal_(self.key_conv.weight, mode='fan_out', nonlinearity='relu')\n",
    "        init.kaiming_normal_(self.query_conv.weight, mode='fan_out', nonlinearity='relu')\n",
    "        for _ in self.value_conv:\n",
    "            init.kaiming_normal_(_.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "        init.normal_(self.emb_a, 0, 1)\n",
    "        init.normal_(self.emb_b, 0, 1)\n",
    "        init.normal_(self.emb_mix, 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T10:45:41.893623Z",
     "iopub.status.busy": "2021-12-07T10:45:41.893223Z",
     "iopub.status.idle": "2021-12-07T10:45:41.918504Z",
     "shell.execute_reply": "2021-12-07T10:45:41.917641Z",
     "shell.execute_reply.started": "2021-12-07T10:45:41.893587Z"
    }
   },
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, groups=1, base_width=64):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.stride = stride\n",
    "        width = int(out_channels * (base_width / 64.)) * groups\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, width, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(width),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            AttentionConv(width, width, kernel_size=7, padding=3, groups=8),\n",
    "            nn.BatchNorm2d(width),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(width, self.expansion * out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(self.expansion * out_channels),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != self.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, self.expansion * out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "        if self.stride >= 2:\n",
    "            out = F.avg_pool2d(out, (self.stride, self.stride))\n",
    "\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=1000, stem=False):\n",
    "        super(Model, self).__init__()\n",
    "        self.in_places = 64\n",
    "\n",
    "        if stem:\n",
    "            self.init = nn.Sequential(\n",
    "                # CIFAR10\n",
    "                AttentionStem(in_channels=3, out_channels=64, kernel_size=4, stride=1, padding=2, groups=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                # For ImageNet\n",
    "                # AttentionStem(in_channels=3, out_channels=64, kernel_size=4, stride=1, padding=2, groups=1),\n",
    "                # nn.BatchNorm2d(64),\n",
    "                # nn.ReLU(),\n",
    "                # nn.MaxPool2d(4, 4)\n",
    "            )\n",
    "        else:\n",
    "            self.init = nn.Sequential(\n",
    "                # CIFAR10\n",
    "                # nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                # nn.BatchNorm2d(64),\n",
    "                # nn.ReLU(),\n",
    "\n",
    "                # For ImageNet\n",
    "                nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            )\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.dense = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_places, planes, stride))\n",
    "            self.in_places = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.init(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.dense(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet26(num_classes=1000, stem=False):\n",
    "    return Model(Bottleneck, [1, 2, 4, 1], num_classes=num_classes, stem=stem)\n",
    "\n",
    "\n",
    "def ResNet38(num_classes=1000, stem=False):\n",
    "    return Model(Bottleneck, [2, 3, 5, 2], num_classes=num_classes, stem=stem)\n",
    "\n",
    "\n",
    "def ResNet50(num_classes=1000, stem=False):\n",
    "    return Model(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, stem=stem)\n",
    "\n",
    "\n",
    "def get_model_parameters(model):\n",
    "    total_parameters = 0\n",
    "    for layer in list(model.parameters()):\n",
    "        layer_parameter = 1\n",
    "        for l in list(layer.size()):\n",
    "            layer_parameter *= l\n",
    "        total_parameters += layer_parameter\n",
    "    return total_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T10:45:41.920036Z",
     "iopub.status.busy": "2021-12-07T10:45:41.919698Z",
     "iopub.status.idle": "2021-12-07T10:45:42.116208Z",
     "shell.execute_reply": "2021-12-07T10:45:42.11533Z",
     "shell.execute_reply.started": "2021-12-07T10:45:41.920002Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is the stand alone self-attention model\n",
    "\"\"\"\n",
    "model_Attention = ResNet50(num_classes=2,stem=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T10:45:42.117977Z",
     "iopub.status.busy": "2021-12-07T10:45:42.117631Z",
     "iopub.status.idle": "2021-12-07T10:45:42.155408Z",
     "shell.execute_reply": "2021-12-07T10:45:42.154538Z",
     "shell.execute_reply.started": "2021-12-07T10:45:42.117943Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is the our modified byol with colorization\n",
    "\"\"\"\n",
    "\n",
    "def default(val, def_val):\n",
    "    return def_val if val is None else val\n",
    "\n",
    "def flatten(t):\n",
    "    return t.reshape(t.shape[0], -1)\n",
    "\n",
    "def singleton(cache_key):\n",
    "    def inner_fn(fn):\n",
    "        @wraps(fn)\n",
    "        def wrapper(self, *args, **kwargs):\n",
    "            instance = getattr(self, cache_key)\n",
    "            if instance is not None:\n",
    "                return instance\n",
    "\n",
    "            instance = fn(self, *args, **kwargs)\n",
    "            setattr(self, cache_key, instance)\n",
    "            return instance\n",
    "        return wrapper\n",
    "    return inner_fn\n",
    "\n",
    "def get_module_device(module):\n",
    "    return next(module.parameters()).device\n",
    "\n",
    "def set_requires_grad(model, val):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = val\n",
    "\n",
    "def loss_fn(x, y):\n",
    "    x = F.normalize(x, dim=1)\n",
    "    y = F.normalize(y, dim=1)\n",
    "    return 2 - 2 * (x * y).sum(dim=-1)\n",
    "\n",
    "# augmentation utils\n",
    "\n",
    "class RandomApply(nn.Module):\n",
    "    def __init__(self, fn, p):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.p = p\n",
    "    def forward(self, x):\n",
    "        if random.random() > self.p:\n",
    "            return x\n",
    "        return self.fn(x)\n",
    "\n",
    "# exponential moving average\n",
    "\n",
    "class EMA():\n",
    "    def __init__(self, beta):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "\n",
    "    def update_average(self, old, new):\n",
    "        if old is None:\n",
    "            return new\n",
    "        return old * self.beta + (1 - self.beta) * new\n",
    "\n",
    "def update_moving_average(ema_updater, ma_model, current_model):\n",
    "    for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n",
    "        old_weight, up_weight = ma_params.data, current_params.data\n",
    "        ma_params.data = ema_updater.update_average(old_weight, up_weight)\n",
    "\n",
    "        \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim, projection_size, hidden_size = 4096):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_size, projection_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class NetWrapper(nn.Module):\n",
    "    def __init__(self, net, projection_size, projection_hidden_size, layer = -2):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.layer = layer\n",
    "\n",
    "        self.projector = None\n",
    "        self.projection_size = projection_size\n",
    "        self.projection_hidden_size = projection_hidden_size\n",
    "\n",
    "        self.hidden = {}\n",
    "        self.hook_registered = False\n",
    "\n",
    "    def _find_layer(self):\n",
    "        if type(self.layer) == str:\n",
    "            modules = dict([*self.net.named_modules()])\n",
    "            return modules.get(self.layer, None)\n",
    "        elif type(self.layer) == int:\n",
    "            children = [*self.net.children()]\n",
    "            return children[self.layer]\n",
    "        return None\n",
    "\n",
    "    def _hook(self, _, input, output):\n",
    "        device = input[0].device\n",
    "        self.hidden[device] = flatten(output)\n",
    "\n",
    "    def _register_hook(self):\n",
    "        layer = self._find_layer()\n",
    "        assert layer is not None, f'hidden layer ({self.layer}) not found'\n",
    "        handle = layer.register_forward_hook(self._hook)\n",
    "        self.hook_registered = True\n",
    "\n",
    "    @singleton('projector')\n",
    "    def _get_projector(self, hidden):\n",
    "        _, dim = hidden.shape\n",
    "        projector = MLP(dim, self.projection_size, self.projection_hidden_size)\n",
    "        return projector.to(hidden)\n",
    "   \n",
    "    def get_representation(self, x):\n",
    "        if self.layer == -1:\n",
    "            return self.net(x)\n",
    "\n",
    "        if not self.hook_registered:\n",
    "            self._register_hook()\n",
    "\n",
    "        self.hidden.clear()\n",
    "        _ = self.net(x)\n",
    "        hidden = self.hidden[x.device]\n",
    "        self.hidden.clear()\n",
    "\n",
    "        assert hidden is not None, f'hidden layer {self.layer} never emitted an output'\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, x, return_projection = True):\n",
    "        representation = self.get_representation(x)\n",
    "\n",
    "        if not return_projection:\n",
    "            return representation\n",
    "\n",
    "        projector = self._get_projector(representation)\n",
    "        projection = projector(representation)\n",
    "        return projection, representation\n",
    "class BYOL(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        net,\n",
    "        image_size,\n",
    "        hidden_layer = -2,\n",
    "        projection_size = 256,\n",
    "        projection_hidden_size = 4096,\n",
    "        augment_fn = None,\n",
    "        augment_fn2 = None,\n",
    "        moving_average_decay = 0.99,\n",
    "        use_momentum = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "\n",
    "        # default SimCLR augmentation\n",
    "\n",
    "        grayscale_aug = torch.nn.Sequential(\n",
    "            T.GaussianBlur((3, 3), (1.0, 2.0))\n",
    "        )\n",
    "\n",
    "        colored_aug = torch.nn.Sequential(\n",
    "            RandomApply(T.ColorJitter(0.8,0.8,0.8,0.2),p=0.3),\n",
    "            T.GaussianBlur((3, 3), (1.0, 2.0)),\n",
    "            T.RandomHorizontalFlip()\n",
    "        )\n",
    "\n",
    "#         self.augment1 = default(augment_fn, grayscale_aug)\n",
    "#         self.augment2 = default(augment_fn2,colored_aug)\n",
    "        self.online_encoder = NetWrapper(net, projection_size, projection_hidden_size, layer=hidden_layer)\n",
    "\n",
    "        self.use_momentum = use_momentum\n",
    "        self.target_encoder = None\n",
    "        self.target_ema_updater = EMA(moving_average_decay)\n",
    "\n",
    "        self.online_predictor = MLP(projection_size, projection_size, projection_hidden_size)\n",
    "\n",
    "        # get device of network and make wrapper same device\n",
    "        device = get_module_device(net)\n",
    "        self.to(device)\n",
    "\n",
    "        # send a mock image tensor to instantiate singleton parameters\n",
    "        self.forward(torch.randn(2, 3, image_size, image_size, device=device),torch.randn(2,3,image_size,image_size,device=device))\n",
    "\n",
    "    @singleton('target_encoder')\n",
    "    def _get_target_encoder(self):\n",
    "        target_encoder = copy.deepcopy(self.online_encoder)\n",
    "        set_requires_grad(target_encoder, False)\n",
    "        return target_encoder\n",
    "\n",
    "    def reset_moving_average(self):\n",
    "        del self.target_encoder\n",
    "        self.target_encoder = None\n",
    "\n",
    "    def update_moving_average(self):\n",
    "        assert self.use_momentum, 'you do not need to update the moving average, since you have turned off momentum for the target encoder'\n",
    "        assert self.target_encoder is not None, 'target encoder has not been created yet'\n",
    "        update_moving_average(self.target_ema_updater, self.target_encoder, self.online_encoder)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,y,\n",
    "        return_embedding = False,\n",
    "        return_projection = True\n",
    "    ):\n",
    "        if return_embedding:\n",
    "            return self.online_encoder(x, return_projection = return_projection)\n",
    "\n",
    "        # image_one, image_two = self.augment1(x), x\n",
    "#         image_one,image_two = self.augment1(y), self.augment2(x)\n",
    "        image_one, image_two = y,x\n",
    "\n",
    "\n",
    "        online_proj_one, _ = self.online_encoder(image_one)\n",
    "        online_proj_two, _ = self.online_encoder(image_two)\n",
    "\n",
    "        online_pred_one = self.online_predictor(online_proj_one)\n",
    "        online_pred_two = self.online_predictor(online_proj_two)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target_encoder = self._get_target_encoder() if self.use_momentum else self.online_encoder\n",
    "            target_proj_one, _ = target_encoder(image_one)\n",
    "            target_proj_two, _ = target_encoder(image_two)\n",
    "            target_proj_one.detach_()\n",
    "            target_proj_two.detach_()\n",
    "\n",
    "        loss_one = loss_fn(online_pred_one, target_proj_two.detach())\n",
    "        loss_two = loss_fn(online_pred_two, target_proj_one.detach())\n",
    "\n",
    "        loss = loss_one + loss_two\n",
    "        return loss.mean()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T10:45:42.157122Z",
     "iopub.status.busy": "2021-12-07T10:45:42.156759Z",
     "iopub.status.idle": "2021-12-07T10:45:57.055469Z",
     "shell.execute_reply": "2021-12-07T10:45:57.054551Z",
     "shell.execute_reply.started": "2021-12-07T10:45:42.157085Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is the original byol\n",
    "\"\"\"\n",
    "\n",
    "import copy\n",
    "import random\n",
    "from functools import wraps\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import transforms as T\n",
    "\n",
    "# helper functions\n",
    "\n",
    "def default(val, def_val):\n",
    "    return def_val if val is None else val\n",
    "\n",
    "def flatten(t):\n",
    "    return t.reshape(t.shape[0], -1)\n",
    "\n",
    "def singleton(cache_key):\n",
    "    def inner_fn(fn):\n",
    "        @wraps(fn)\n",
    "        def wrapper(self, *args, **kwargs):\n",
    "            instance = getattr(self, cache_key)\n",
    "            if instance is not None:\n",
    "                return instance\n",
    "\n",
    "            instance = fn(self, *args, **kwargs)\n",
    "            setattr(self, cache_key, instance)\n",
    "            return instance\n",
    "        return wrapper\n",
    "    return inner_fn\n",
    "\n",
    "def get_module_device(module):\n",
    "    return next(module.parameters()).device\n",
    "\n",
    "def set_requires_grad(model, val):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = val\n",
    "\n",
    "# loss fn\n",
    "\n",
    "def loss_fn(x, y):\n",
    "    x = F.normalize(x, dim=-1, p=2)\n",
    "    y = F.normalize(y, dim=-1, p=2)\n",
    "    return 2 - 2 * (x * y).sum(dim=-1)\n",
    "\n",
    "# augmentation utils\n",
    "\n",
    "class RandomApply(nn.Module):\n",
    "    def __init__(self, fn, p):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.p = p\n",
    "    def forward(self, x):\n",
    "        if random.random() > self.p:\n",
    "            return x\n",
    "        return self.fn(x)\n",
    "\n",
    "# exponential moving average\n",
    "\n",
    "class EMA():\n",
    "    def __init__(self, beta):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "\n",
    "    def update_average(self, old, new):\n",
    "        if old is None:\n",
    "            return new\n",
    "        return old * self.beta + (1 - self.beta) * new\n",
    "\n",
    "def update_moving_average(ema_updater, ma_model, current_model):\n",
    "    for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n",
    "        old_weight, up_weight = ma_params.data, current_params.data\n",
    "        ma_params.data = ema_updater.update_average(old_weight, up_weight)\n",
    "\n",
    "# MLP class for projector and predictor\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim, projection_size, hidden_size = 4096):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_size, projection_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# a wrapper class for the base neural network\n",
    "# will manage the interception of the hidden layer output\n",
    "# and pipe it into the projecter and predictor nets\n",
    "\n",
    "class NetWrapper(nn.Module):\n",
    "    def __init__(self, net, projection_size, projection_hidden_size, layer = -2):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.layer = layer\n",
    "\n",
    "        self.projector = None\n",
    "        self.projection_size = projection_size\n",
    "        self.projection_hidden_size = projection_hidden_size\n",
    "\n",
    "        self.hidden = {}\n",
    "        self.hook_registered = False\n",
    "\n",
    "    def _find_layer(self):\n",
    "        if type(self.layer) == str:\n",
    "            modules = dict([*self.net.named_modules()])\n",
    "            return modules.get(self.layer, None)\n",
    "        elif type(self.layer) == int:\n",
    "            children = [*self.net.children()]\n",
    "            return children[self.layer]\n",
    "        return None\n",
    "\n",
    "    def _hook(self, _, input, output):\n",
    "        device = input[0].device\n",
    "        self.hidden[device] = flatten(output)\n",
    "\n",
    "    def _register_hook(self):\n",
    "        layer = self._find_layer()\n",
    "        assert layer is not None, f'hidden layer ({self.layer}) not found'\n",
    "        handle = layer.register_forward_hook(self._hook)\n",
    "        self.hook_registered = True\n",
    "\n",
    "    @singleton('projector')\n",
    "    def _get_projector(self, hidden):\n",
    "        _, dim = hidden.shape\n",
    "        projector = MLP(dim, self.projection_size, self.projection_hidden_size)\n",
    "        return projector.to(hidden)\n",
    "\n",
    "    def get_representation(self, x):\n",
    "        if self.layer == -1:\n",
    "            return self.net(x)\n",
    "\n",
    "        if not self.hook_registered:\n",
    "            self._register_hook()\n",
    "\n",
    "        self.hidden.clear()\n",
    "        _ = self.net(x)\n",
    "        hidden = self.hidden[x.device]\n",
    "        self.hidden.clear()\n",
    "\n",
    "        assert hidden is not None, f'hidden layer {self.layer} never emitted an output'\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, x, return_projection = True):\n",
    "        representation = self.get_representation(x)\n",
    "\n",
    "        if not return_projection:\n",
    "            return representation\n",
    "\n",
    "        projector = self._get_projector(representation)\n",
    "        projection = projector(representation)\n",
    "        return projection, representation\n",
    "\n",
    "# main class\n",
    "\n",
    "class BYOL_original(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        net,\n",
    "        image_size,\n",
    "        hidden_layer = -2,\n",
    "        projection_size = 256,\n",
    "        projection_hidden_size = 4096,\n",
    "        augment_fn = None,\n",
    "        augment_fn2 = None,\n",
    "        moving_average_decay = 0.99,\n",
    "        use_momentum = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "\n",
    "        # default SimCLR augmentation\n",
    "\n",
    "        DEFAULT_AUG = torch.nn.Sequential(\n",
    "            RandomApply(\n",
    "                T.ColorJitter(0.8, 0.8, 0.8, 0.2),\n",
    "                p = 0.3\n",
    "            ),\n",
    "            T.RandomGrayscale(p=0.2),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            RandomApply(\n",
    "                T.GaussianBlur((3, 3), (1.0, 2.0)),\n",
    "                p = 0.2\n",
    "            ),\n",
    "            T.RandomResizedCrop((image_size, image_size)),\n",
    "            T.Normalize(\n",
    "                mean=torch.tensor([0.485, 0.456, 0.406]),\n",
    "                std=torch.tensor([0.229, 0.224, 0.225])),\n",
    "        )\n",
    "\n",
    "        self.augment1 = default(augment_fn, DEFAULT_AUG)\n",
    "        self.augment2 = default(augment_fn2, self.augment1)\n",
    "\n",
    "        self.online_encoder = NetWrapper(net, projection_size, projection_hidden_size, layer=hidden_layer)\n",
    "\n",
    "        self.use_momentum = use_momentum\n",
    "        self.target_encoder = None\n",
    "        self.target_ema_updater = EMA(moving_average_decay)\n",
    "\n",
    "        self.online_predictor = MLP(projection_size, projection_size, projection_hidden_size)\n",
    "\n",
    "        # get device of network and make wrapper same device\n",
    "        device = get_module_device(net)\n",
    "        self.to(device)\n",
    "\n",
    "        # send a mock image tensor to instantiate singleton parameters\n",
    "        self.forward(torch.randn(2, 3, image_size, image_size, device=device))\n",
    "\n",
    "    @singleton('target_encoder')\n",
    "    def _get_target_encoder(self):\n",
    "        target_encoder = copy.deepcopy(self.online_encoder)\n",
    "        set_requires_grad(target_encoder, False)\n",
    "        return target_encoder\n",
    "\n",
    "    def reset_moving_average(self):\n",
    "        del self.target_encoder\n",
    "        self.target_encoder = None\n",
    "\n",
    "    def update_moving_average(self):\n",
    "        assert self.use_momentum, 'you do not need to update the moving average, since you have turned off momentum for the target encoder'\n",
    "        assert self.target_encoder is not None, 'target encoder has not been created yet'\n",
    "        update_moving_average(self.target_ema_updater, self.target_encoder, self.online_encoder)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        return_embedding = False,\n",
    "        return_projection = True\n",
    "    ):\n",
    "        assert not (self.training and x.shape[0] == 1), 'you must have greater than 1 sample when training, due to the batchnorm in the projection layer'\n",
    "\n",
    "        if return_embedding:\n",
    "            return self.online_encoder(x, return_projection = return_projection)\n",
    "\n",
    "        image_one, image_two = self.augment1(x), self.augment2(x)\n",
    "\n",
    "        online_proj_one, _ = self.online_encoder(image_one)\n",
    "        online_proj_two, _ = self.online_encoder(image_two)\n",
    "\n",
    "        online_pred_one = self.online_predictor(online_proj_one)\n",
    "        online_pred_two = self.online_predictor(online_proj_two)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target_encoder = self._get_target_encoder() if self.use_momentum else self.online_encoder\n",
    "            target_proj_one, _ = target_encoder(image_one)\n",
    "            target_proj_two, _ = target_encoder(image_two)\n",
    "            target_proj_one.detach_()\n",
    "            target_proj_two.detach_()\n",
    "\n",
    "        loss_one = loss_fn(online_pred_one, target_proj_two.detach())\n",
    "        loss_two = loss_fn(online_pred_two, target_proj_one.detach())\n",
    "\n",
    "        loss = loss_one + loss_two\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "model_attention = ResNet50(num_classes=2,stem=False)    \n",
    "resnet= models.resnet50(pretrained=True)\n",
    "\n",
    "#creating object of BYOL method with resnet encoder\n",
    "resnet_byol_original = BYOL_original(resnet,image_size=128).to(device)\n",
    "\n",
    "#creating object of byol method with self-attention encoder\n",
    "attention_byol_original = BYOL_original(model_attention,image_size=128).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T10:45:57.057097Z",
     "iopub.status.busy": "2021-12-07T10:45:57.05675Z",
     "iopub.status.idle": "2021-12-07T10:46:37.623623Z",
     "shell.execute_reply": "2021-12-07T10:46:37.622727Z",
     "shell.execute_reply.started": "2021-12-07T10:45:57.05706Z"
    }
   },
   "outputs": [],
   "source": [
    "# new_resnet_wts = '/kaggle/input/new-resnet-wts/new_byol_resnet_sgd_4e3_L2loss_20epoch.pt'\n",
    "# new_resnet_wts = '/kaggle/input/segmentation-dataset/new_byol_resnet_sgd_4e3_L2loss_50epoch.pt'\n",
    "\n",
    "\"\"\"\n",
    "set path for the pre-trained weights obtained after the pre-training\n",
    "\"\"\"\n",
    "\n",
    "#weights of resnet50 encoder pre-trained with our method\n",
    "new_resnet_wts = '/kaggle/input/new-byol-100epochs/new_byol_resnet_sgd_4e3_L2loss_100epoch.pt'\n",
    "\n",
    "#weights of self-attention encoder pre-trained with our method\n",
    "attention_wts = '../input/new-byol-attention-100epochs/new_byol_Attention_adam_4e3_L2loss_100epoch.pt'\n",
    "\n",
    "#weights of resnet50 encoder pre-trained with original byol\n",
    "old_byol_resnet = \"../input/old-byol-resnet/old_byol_resnet_adam_4e3_L2loss_50epoch.pt\"\n",
    "\n",
    "#weights of self-attention encoder pre-trained with original byol \n",
    "old_byol_attention = '../input/original-byol-attention-50epochs/old_byol_attention_adam_4e3_L2loss_50epoch.pt'\n",
    "\n",
    "#creating object of resnet50 encoder with Imagenet weights\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "\n",
    "#creating object of our method with resnet encoder\n",
    "new_byol_resnet = BYOL(resnet, image_size = 128, hidden_layer = 'avgpool').to(device)\n",
    "\n",
    "#loading the pre-trained weights in the object created above\n",
    "chkpoint = torch.load(new_resnet_wts)\n",
    "new_byol_resnet.load_state_dict(chkpoint['model_state_dict'])\n",
    "new_byol_resnet = new_byol_resnet.to(device)\n",
    "\n",
    "\n",
    "#creating an object of our method with self-attention encoder and loading the pre-trained weights\n",
    "new_byol_attention = BYOL(model_Attention,image_size = 128).to(device)\n",
    "chkpoint_attention = torch.load(attention_wts)\n",
    "new_byol_attention.load_state_dict(chkpoint_attention['model_state_dict'])\n",
    "new_byol_attention = new_byol_attention.to(device)\n",
    "\n",
    "\n",
    "#loading pre-trained weights into the self-attention byol architecture\n",
    "chkpoint_original_attention = torch.load(old_byol_attention)\n",
    "attention_byol_original.load_state_dict(chkpoint_original_attention['model_state_dict'])\n",
    "attention_byol_original = attention_byol_original.to(device)\n",
    "\n",
    "#loading pre-trained weights into the resnet50 byol architecture\n",
    "chkpoint_original_resnet = torch.load(old_byol_resnet)\n",
    "resnet_byol_original.load_state_dict(chkpoint_original_resnet['model_state_dict'])\n",
    "resnet_byol_original = resnet_byol_original.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T10:46:37.625165Z",
     "iopub.status.busy": "2021-12-07T10:46:37.624821Z",
     "iopub.status.idle": "2021-12-07T10:46:37.633009Z",
     "shell.execute_reply": "2021-12-07T10:46:37.630616Z",
     "shell.execute_reply.started": "2021-12-07T10:46:37.62513Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here, the online encoder from the objects of the byol architecture is extracted.\n",
    "\"\"\"\n",
    "\n",
    "new_encoder_Resnet = new_byol_resnet.online_encoder\n",
    "new_encoder_Resnet = torch.nn.Sequential(*list(new_encoder_Resnet.children())[:-1])\n",
    "\n",
    "new_encoder_attention = new_byol_attention.online_encoder\n",
    "new_encoder_attention = torch.nn.Sequential(*list(new_encoder_attention.children())[:-1])\n",
    "\n",
    "# old_encoder_Resnet = resnet_byol_original.online_encoder\n",
    "# old_encoder_Resnet = torch.nn.Sequential(*list(old_encoder_Resnet.children())[:-1])\n",
    "\n",
    "attention_byol_original_encoder = attention_byol_original.online_encoder\n",
    "attention_byol_original_encoder = torch.nn.Sequential(*list(attention_byol_original_encoder.children())[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T10:46:37.63542Z",
     "iopub.status.busy": "2021-12-07T10:46:37.634676Z",
     "iopub.status.idle": "2021-12-07T10:46:37.642831Z",
     "shell.execute_reply": "2021-12-07T10:46:37.642035Z",
     "shell.execute_reply.started": "2021-12-07T10:46:37.635381Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part achieves the task of freezing the encoder extracted in the last cell\n",
    "\"\"\"\n",
    "\n",
    "# for param in new_encoder_Resnet.parameters():\n",
    "#     param.requires_grad = False\n",
    "    \n",
    "# for param in new_encoder_attention.parameters():\n",
    "#     param.requires_grad = False\n",
    "    \n",
    "# for param in old_encoder_Resnet.parameters():\n",
    "#     param.requires_grad = False\n",
    "    \n",
    "# for param in attention_byol_original_encoder.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T10:46:37.646107Z",
     "iopub.status.busy": "2021-12-07T10:46:37.645811Z",
     "iopub.status.idle": "2021-12-07T10:46:37.666842Z",
     "shell.execute_reply": "2021-12-07T10:46:37.665832Z",
     "shell.execute_reply.started": "2021-12-07T10:46:37.646081Z"
    }
   },
   "outputs": [],
   "source": [
    "#U-net architecture\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "    \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024 // factor)\n",
    "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
    "        self.up2 = Up(512, 256 // factor, bilinear)\n",
    "        self.up3 = Up(256, 128 // factor, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n",
    "#     Unet_original = UNet(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T10:46:37.669358Z",
     "iopub.status.busy": "2021-12-07T10:46:37.668639Z",
     "iopub.status.idle": "2021-12-07T10:46:37.735698Z",
     "shell.execute_reply": "2021-12-07T10:46:37.734831Z",
     "shell.execute_reply.started": "2021-12-07T10:46:37.669265Z"
    }
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Helper module that consists of a Conv -> BN -> ReLU\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, padding=1, kernel_size=3, stride=1, with_nonlinearity=True):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, padding=padding, kernel_size=kernel_size, stride=stride)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.with_nonlinearity = with_nonlinearity\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        if self.with_nonlinearity:\n",
    "            x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Bridge(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the middle layer of the UNet which just consists of some\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.bridge = nn.Sequential(\n",
    "            ConvBlock(in_channels, out_channels),\n",
    "            ConvBlock(out_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.bridge(x)\n",
    "\n",
    "class UpBlockForUNetWithResNet50Color(nn.Module):\n",
    "    \"\"\"\n",
    "    Up block that encapsulates one up-sampling step which consists of Upsample -> ConvBlock -> ConvBlock\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, up_conv_in_channels=None, up_conv_out_channels=None,\n",
    "                 upsampling_method=\"conv_transpose\"):\n",
    "        super().__init__()\n",
    "\n",
    "        if up_conv_in_channels == None:\n",
    "            up_conv_in_channels = in_channels\n",
    "        if up_conv_out_channels == None:\n",
    "            up_conv_out_channels = out_channels\n",
    "\n",
    "        if upsampling_method == \"conv_transpose\":\n",
    "            self.upsample = nn.ConvTranspose2d(up_conv_in_channels, up_conv_out_channels, kernel_size=2, stride=2)\n",
    "        elif upsampling_method == \"bilinear\":\n",
    "            self.upsample = nn.Sequential(\n",
    "                nn.Upsample(mode='bilinear', scale_factor=2),\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1)\n",
    "            )\n",
    "        self.conv_block_1 = ConvBlock(in_channels, out_channels)\n",
    "        self.conv_block_2 = ConvBlock(out_channels, out_channels)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, up_x, down_x):\n",
    "        \"\"\"\n",
    "        :param up_x: this is the output from the previous up block\n",
    "        :param down_x: this is the output from the down block\n",
    "        :return: upsampled feature map\n",
    "        \"\"\"\n",
    "        x = self.upsample(up_x)\n",
    "        # x = torch.cat([x, down_x], 1)\n",
    "        # x = self.conv_block_1(x)\n",
    "        # x = self.conv_block_2(x)\n",
    "        return x\n",
    "    \n",
    "\"\"\"\n",
    "This is the Upsampling class for resnet50 encoder\n",
    "\"\"\"    \n",
    "class UpBlockForUNetWithResNet50(nn.Module):\n",
    "    \"\"\"\n",
    "    Up block that encapsulates one up-sampling step which consists of Upsample -> ConvBlock -> ConvBlock\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, up_conv_in_channels=None, up_conv_out_channels=None,\n",
    "                 upsampling_method=\"conv_transpose\"):\n",
    "        super().__init__()\n",
    "\n",
    "        if up_conv_in_channels == None:\n",
    "            up_conv_in_channels = in_channels\n",
    "        if up_conv_out_channels == None:\n",
    "            up_conv_out_channels = out_channels\n",
    "\n",
    "        if upsampling_method == \"conv_transpose\":\n",
    "            self.upsample = nn.ConvTranspose2d(up_conv_in_channels, up_conv_out_channels, kernel_size=2, stride=2)\n",
    "        elif upsampling_method == \"bilinear\":\n",
    "            self.upsample = nn.Sequential(\n",
    "                nn.Upsample(mode='bilinear', scale_factor=2),\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1)\n",
    "            )\n",
    "        self.conv_block_1 = ConvBlock(in_channels, out_channels)\n",
    "        self.conv_block_2 = ConvBlock(out_channels, out_channels)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, up_x, down_x):\n",
    "        \"\"\"\n",
    "        :param up_x: this is the output from the previous up block\n",
    "        :param down_x: this is the output from the down block\n",
    "        :return: upsampled feature map\n",
    "        \"\"\"\n",
    "        x = self.upsample(up_x)\n",
    "        x = torch.cat([x, down_x], 1)\n",
    "        x = self.conv_block_1(x)\n",
    "        x = self.conv_block_2(x)\n",
    "        return x\n",
    "\n",
    "\"\"\"\n",
    "This is the Unet class for self-attention encoder\n",
    "\"\"\"\n",
    "class UNetWithAttentionEncoder(nn.Module):\n",
    "    Depth = 6\n",
    "    \n",
    "    def __init__(self,byol_model,n_classes=2):\n",
    "        super().__init__()\n",
    "        down_blocks = []\n",
    "        up_blocks = []\n",
    "        # for child in model_Attention.children():\n",
    "        #   if child.init:\n",
    "        #     conv1 = child.init[0]\n",
    "        #     bn1 = child.init[1]\n",
    "        #     relu1 = child.init[2]\n",
    "        #     self.input_block = nn.Sequential(*[conv1,bn1,relu1])\n",
    "        #     self.input_pool = child.init[3]\n",
    "\n",
    "        #   if child.layer1:\n",
    "        #     down_blocks.append(child.layer1)\n",
    "\n",
    "        #   if child.layer2:\n",
    "        #     down_blocks.append(child.layer2)\n",
    "\n",
    "        #   if child.layer3:\n",
    "        #     down_blocks.append(child.layer3)\n",
    "\n",
    "        #   if child.layer4:\n",
    "        #     down_blocks.append(child.layer4)\n",
    "\n",
    "        self.input_block = nn.Sequential(*list(byol_model.children())[0][:3])\n",
    "        self.input_pool = list(byol_model.children())[0][3]\n",
    "\n",
    "        for bottleneck in list(byol_model.children())[1:]:\n",
    "            if isinstance(bottleneck, nn.Sequential):\n",
    "                down_blocks.append(bottleneck)\n",
    "\n",
    "        self.down_blocks = nn.ModuleList(down_blocks)\n",
    "        self.bridge = Bridge(2048, 2048)\n",
    "        up_blocks.append(UpBlockForUNetWithResNet50(2048, 1024))\n",
    "        up_blocks.append(UpBlockForUNetWithResNet50(1024, 512))\n",
    "        up_blocks.append(UpBlockForUNetWithResNet50(512, 256))\n",
    "        up_blocks.append(UpBlockForUNetWithResNet50(in_channels=128 + 64, out_channels=128,\n",
    "                                                    up_conv_in_channels=256, up_conv_out_channels=128))\n",
    "        \n",
    "        \"\"\"\n",
    "        Here , we change the value of in_channel in the Upblock as per the input. If it is a grayscale image in_channel=64+1 and if it is colored image then in_channel = 64+1 \n",
    "        \"\"\"\n",
    "        up_blocks.append(UpBlockForUNetWithResNet50(in_channels=64 + 3, out_channels=64,\n",
    "                                                    up_conv_in_channels=128, up_conv_out_channels=64))\n",
    "\n",
    "        self.up_blocks = nn.ModuleList(up_blocks)\n",
    "\n",
    "        self.out = nn.Conv2d(64, 1, kernel_size=1, stride=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, with_output_feature_map=False):\n",
    "        pre_pools = dict()\n",
    "        pre_pools[f\"layer_0\"] = x\n",
    "        x = self.input_block(x)\n",
    "        pre_pools[f\"layer_1\"] = x\n",
    "        x = self.input_pool(x)\n",
    "\n",
    "        for i, block in enumerate(self.down_blocks, 2):\n",
    "            \n",
    "          # print(block)\n",
    "            x = block(x)\n",
    "            if i == (UNetWithResnet50Encoder.DEPTH - 1):\n",
    "                continue\n",
    "            pre_pools[f\"layer_{i}\"] = x\n",
    "\n",
    "        x = self.bridge(x)\n",
    "\n",
    "        for i, block in enumerate(self.up_blocks, 1):\n",
    "            key = f\"layer_{UNetWithResnet50Encoder.DEPTH - 1 - i}\"\n",
    "            x = block(x, pre_pools[key])\n",
    "        output_feature_map = x\n",
    "        x = self.out(x)\n",
    "        del pre_pools\n",
    "        if with_output_feature_map:\n",
    "            return x, output_feature_map\n",
    "        else:\n",
    "            return self.sigmoid(x)\n",
    "\n",
    "\"\"\"\n",
    "This is the Unet class for self-attention encoder\n",
    "\"\"\"\n",
    "class UNetWithResnet50Encoder(nn.Module):\n",
    "    DEPTH = 6\n",
    "\n",
    "    def __init__(self, resnet, n_classes=2):\n",
    "        super().__init__()\n",
    "        down_blocks = []\n",
    "        up_blocks = []\n",
    "        # resnet = torchvision.models.resnet.resnet50(pretrained=True)\n",
    "\n",
    "        self.input_block = nn.Sequential(*list(resnet.children()))[:3]\n",
    "        self.input_pool = list(resnet.children())[3]\n",
    "\n",
    "        for bottleneck in list(resnet.children()):\n",
    "            if isinstance(bottleneck, nn.Sequential):\n",
    "                down_blocks.append(bottleneck)\n",
    "                  \n",
    "        self.down_blocks = nn.ModuleList(down_blocks)\n",
    "        self.bridge = Bridge(2048, 2048)\n",
    "        up_blocks.append(UpBlockForUNetWithResNet50(2048, 1024))\n",
    "        up_blocks.append(UpBlockForUNetWithResNet50(1024, 512))\n",
    "        up_blocks.append(UpBlockForUNetWithResNet50(512, 256))\n",
    "        up_blocks.append(UpBlockForUNetWithResNet50(in_channels=128 + 64, out_channels=128,\n",
    "                                                    up_conv_in_channels=256, up_conv_out_channels=128))\n",
    "        \n",
    "        \"\"\"\n",
    "        Here , we change the value of in_channel in the Upblock as per the input. If it is a grayscale image in_channel=64+1 and if it is colored image then in_channel = 64+1 \n",
    "        \"\"\"\n",
    "        up_blocks.append(UpBlockForUNetWithResNet50(in_channels=64 + 3, out_channels=64,\n",
    "                                                    up_conv_in_channels=128, up_conv_out_channels=64))\n",
    "\n",
    "        self.up_blocks = nn.ModuleList(up_blocks)\n",
    "\n",
    "        self.out = nn.Conv2d(64, 1, kernel_size=1, stride=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, with_output_feature_map=False):\n",
    "        pre_pools = dict()\n",
    "        pre_pools[f\"layer_0\"] = x\n",
    "        x = self.input_block(x)\n",
    "        pre_pools[f\"layer_1\"] = x\n",
    "        x = self.input_pool(x)\n",
    "\n",
    "        for i, block in enumerate(self.down_blocks, 2):\n",
    "            # print(block)\n",
    "            x = block(x)\n",
    "            if i == (UNetWithResnet50Encoder.DEPTH - 1):\n",
    "                continue\n",
    "            pre_pools[f\"layer_{i}\"] = x\n",
    "\n",
    "        x = self.bridge(x)\n",
    "\n",
    "        for i, block in enumerate(self.up_blocks, 1):\n",
    "            key = f\"layer_{UNetWithResnet50Encoder.DEPTH - 1 - i}\"\n",
    "            x = block(x, pre_pools[key])\n",
    "        output_feature_map = x\n",
    "        x = self.out(x)\n",
    "        del pre_pools\n",
    "        if with_output_feature_map:\n",
    "            return x, output_feature_map\n",
    "        else:\n",
    "            return self.sigmoid(x)\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "This is the Unet class for self-attention encoder obtained from the BYOL(and also our method) architecture\n",
    "\"\"\"        \n",
    "class UNetByolWithAttentionEncoder(nn.Module):\n",
    "  Depth = 6\n",
    "\n",
    "  def __init__(self,model_for_unet_attention,n_classes=2):\n",
    "    super().__init__()\n",
    "    down_blocks = []\n",
    "    up_blocks = []\n",
    "    for child in model_for_unet_attention.children():\n",
    "      if child.init:\n",
    "        conv1 = child.init[0]\n",
    "        bn1 = child.init[1]\n",
    "        relu1 = child.init[2]\n",
    "        self.input_block = nn.Sequential(*[conv1,bn1,relu1])\n",
    "        self.input_pool = child.init[3]\n",
    "\n",
    "      if child.layer1:\n",
    "        down_blocks.append(child.layer1)\n",
    "\n",
    "      if child.layer2:\n",
    "        down_blocks.append(child.layer2)\n",
    "\n",
    "      if child.layer3:\n",
    "        down_blocks.append(child.layer3)\n",
    "\n",
    "      if child.layer4:\n",
    "        down_blocks.append(child.layer4)\n",
    "\n",
    "    self.down_blocks = nn.ModuleList(down_blocks)\n",
    "    self.bridge = Bridge(2048, 2048)\n",
    "    up_blocks.append(UpBlockForUNetWithResNet50(2048, 1024))\n",
    "    up_blocks.append(UpBlockForUNetWithResNet50(1024, 512))\n",
    "    up_blocks.append(UpBlockForUNetWithResNet50(512, 256))\n",
    "    up_blocks.append(UpBlockForUNetWithResNet50(in_channels=128 + 64, out_channels=128,\n",
    "                                                    up_conv_in_channels=256, up_conv_out_channels=128))\n",
    "    \n",
    "            \"\"\"\n",
    "    Here , we change the value of in_channel in the Upblock as per the input. If it is a grayscale image in_channel=64+1 and if it is colored image then in_channel = 64+1 \n",
    "        \"\"\"\n",
    "    up_blocks.append(UpBlockForUNetWithResNet50(in_channels=64 + 3, out_channels=64,\n",
    "                                                    up_conv_in_channels=128, up_conv_out_channels=64))\n",
    "\n",
    "    self.up_blocks = nn.ModuleList(up_blocks)\n",
    "\n",
    "    self.out = nn.Conv2d(64, 1, kernel_size=1, stride=1)\n",
    "    self.sigmoid = nn.Sigmoid()    \n",
    "    \n",
    "  def forward(self, x, with_output_feature_map=False):\n",
    "    pre_pools = dict()\n",
    "    pre_pools[f\"layer_0\"] = x\n",
    "    x = self.input_block(x)\n",
    "    pre_pools[f\"layer_1\"] = x\n",
    "    x = self.input_pool(x)\n",
    "\n",
    "    for i, block in enumerate(self.down_blocks, 2):\n",
    "      x = block(x)\n",
    "      if i == (UNetWithResnet50Encoder.DEPTH - 1):\n",
    "        continue\n",
    "      pre_pools[f\"layer_{i}\"] = x\n",
    "\n",
    "    x = self.bridge(x)\n",
    "\n",
    "    for i, block in enumerate(self.up_blocks, 1):\n",
    "        key = f\"layer_{UNetWithResnet50Encoder.DEPTH - 1 - i}\"\n",
    "        x = block(x, pre_pools[key])\n",
    "    output_feature_map = x\n",
    "    x = self.out(x)\n",
    "    del pre_pools\n",
    "    if with_output_feature_map:\n",
    "      return x, output_feature_map\n",
    "    else:\n",
    "      return self.sigmoid(x)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This is the Unet class for resnet50 encoder obtained from the BYOL(and also our method) architecture\n",
    "\"\"\"\n",
    "class UNetByolWithResnet50Encoder(nn.Module):\n",
    "    DEPTH = 6\n",
    "\n",
    "    def __init__(self,model_for_unet,n_classes=2):\n",
    "        super().__init__()\n",
    "        # resnet = torchvision.models.resnet.resnet50(pretrained=True)\n",
    "        down_blocks = []\n",
    "        up_blocks = []\n",
    "        for child in model_for_unet.children():\n",
    "          if child.conv1:\n",
    "            conv1 = child.conv1\n",
    "          if child.bn1:\n",
    "            bn1 = child.bn1\n",
    "          if child.relu:\n",
    "            relu = child.relu\n",
    "          self.input_block = nn.Sequential(*[conv1,bn1,relu])\n",
    "        # self.input_block = nn.Sequential(*list(resnet.children()))[:3]\n",
    "          # self.input_pool = list(resnet.children())[3]\n",
    "          if child.maxpool:\n",
    "            self.input_pool = child.maxpool\n",
    "\n",
    "          if child.layer1:\n",
    "            down_blocks.append(child.layer1)\n",
    "\n",
    "          if child.layer2:\n",
    "            down_blocks.append(child.layer2)\n",
    "\n",
    "          if child.layer3:\n",
    "            down_blocks.append(child.layer3)\n",
    "\n",
    "          if child.layer4:\n",
    "            down_blocks.append(child.layer4)\n",
    "        # for bottleneck in list(resnet.children()):\n",
    "        #     if isinstance(bottleneck, nn.Sequential):\n",
    "        #         down_blocks.append(bottleneck)\n",
    "                  \n",
    "        self.down_blocks = nn.ModuleList(down_blocks)\n",
    "        self.bridge = Bridge(2048, 2048)\n",
    "        up_blocks.append(UpBlockForUNetWithResNet50(2048, 1024))\n",
    "        up_blocks.append(UpBlockForUNetWithResNet50(1024, 512))\n",
    "        up_blocks.append(UpBlockForUNetWithResNet50(512, 256))\n",
    "        up_blocks.append(UpBlockForUNetWithResNet50(in_channels=128 + 64, out_channels=128,\n",
    "                                                    up_conv_in_channels=256, up_conv_out_channels=128))\n",
    "        \n",
    "        \"\"\"\n",
    "        Here , we change the value of in_channel in the Upblock as per the input. If it is a grayscale image in_channel=64+1 and if it is colored image then in_channel = 64+1 \n",
    "        \"\"\"\n",
    "        up_blocks.append(UpBlockForUNetWithResNet50(in_channels=64 + 3, out_channels=64,\n",
    "                                                    up_conv_in_channels=128, up_conv_out_channels=64))\n",
    "\n",
    "        self.up_blocks = nn.ModuleList(up_blocks)\n",
    "\n",
    "        self.out = nn.Conv2d(64, 1, kernel_size=1, stride=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, with_output_feature_map=False):\n",
    "        pre_pools = dict()\n",
    "        pre_pools[f\"layer_0\"] = x\n",
    "        x = self.input_block(x)\n",
    "        pre_pools[f\"layer_1\"] = x\n",
    "        x = self.input_pool(x)\n",
    "\n",
    "        for i, block in enumerate(self.down_blocks, 2):\n",
    "            x = block(x)\n",
    "            if i == (UNetWithResnet50Encoder.DEPTH - 1):\n",
    "                continue\n",
    "            pre_pools[f\"layer_{i}\"] = x\n",
    "\n",
    "        x = self.bridge(x)\n",
    "\n",
    "        for i, block in enumerate(self.up_blocks, 1):\n",
    "            key = f\"layer_{UNetWithResnet50Encoder.DEPTH - 1 - i}\"\n",
    "            x = block(x, pre_pools[key])\n",
    "        output_feature_map = x\n",
    "        x = self.out(x)\n",
    "        del pre_pools\n",
    "        if with_output_feature_map:\n",
    "            return x, output_feature_map\n",
    "        else:\n",
    "            return self.sigmoid(x)\n",
    "        \n",
    "        \n",
    "class UNetByolWithResnet50EncoderColor(nn.Module):\n",
    "    DEPTH = 6\n",
    "\n",
    "    def __init__(self,model_for_unet,n_classes=2):\n",
    "        super().__init__()\n",
    "        # resnet = torchvision.models.resnet.resnet50(pretrained=True)\n",
    "        down_blocks = []\n",
    "        up_blocks = []\n",
    "        for child in model_for_unet.children():\n",
    "          if child.conv1:\n",
    "            conv1 = child.conv1\n",
    "          if child.bn1:\n",
    "            bn1 = child.bn1\n",
    "          if child.relu:\n",
    "            relu = child.relu\n",
    "          self.input_block = nn.Sequential(*[conv1,bn1,relu])\n",
    "        # self.input_block = nn.Sequential(*list(resnet.children()))[:3]\n",
    "          # self.input_pool = list(resnet.children())[3]\n",
    "          if child.maxpool:\n",
    "            self.input_pool = child.maxpool\n",
    "\n",
    "          if child.layer1:\n",
    "            down_blocks.append(child.layer1)\n",
    "\n",
    "          if child.layer2:\n",
    "            down_blocks.append(child.layer2)\n",
    "\n",
    "          if child.layer3:\n",
    "            down_blocks.append(child.layer3)\n",
    "\n",
    "          if child.layer4:\n",
    "            down_blocks.append(child.layer4)\n",
    "        # for bottleneck in list(resnet.children()):\n",
    "        #     if isinstance(bottleneck, nn.Sequential):\n",
    "        #         down_blocks.append(bottleneck)\n",
    "                  \n",
    "        self.down_blocks = nn.ModuleList(down_blocks)\n",
    "        self.bridge = Bridge(2048, 2048)\n",
    "        up_blocks.append(UpBlockForUNetWithResNet50Color(2048, 1024))\n",
    "        up_blocks.append(UpBlockForUNetWithResNet50Color(1024, 512))\n",
    "        up_blocks.append(UpBlockForUNetWithResNet50Color(512, 256))\n",
    "        up_blocks.append(UpBlockForUNetWithResNet50Color(in_channels=128 + 64, out_channels=128,\n",
    "                                                    up_conv_in_channels=256, up_conv_out_channels=128))\n",
    "        up_blocks.append(UpBlockForUNetWithResNet50Color(in_channels=64 + 1, out_channels=64,\n",
    "                                                    up_conv_in_channels=128, up_conv_out_channels=64))\n",
    "\n",
    "        self.up_blocks = nn.ModuleList(up_blocks)\n",
    "\n",
    "        self.out = nn.Conv2d(64, 3, kernel_size=1, stride=1)\n",
    "\n",
    "    def forward(self, x, with_output_feature_map=False):\n",
    "        pre_pools = dict()\n",
    "        pre_pools[f\"layer_0\"] = x\n",
    "        x = self.input_block(x)\n",
    "        pre_pools[f\"layer_1\"] = x\n",
    "        x = self.input_pool(x)\n",
    "\n",
    "        for i, block in enumerate(self.down_blocks, 2):\n",
    "            x = block(x)\n",
    "            if i == (UNetWithResnet50Encoder.DEPTH - 1):\n",
    "                continue\n",
    "            pre_pools[f\"layer_{i}\"] = x\n",
    "\n",
    "        x = self.bridge(x)\n",
    "\n",
    "        for i, block in enumerate(self.up_blocks, 1):\n",
    "            key = f\"layer_{UNetWithResnet50Encoder.DEPTH - 1 - i}\"\n",
    "            x = block(x, pre_pools[key])\n",
    "        output_feature_map = x\n",
    "        x = self.out(x)\n",
    "        del pre_pools\n",
    "        if with_output_feature_map:\n",
    "            return x, output_feature_map\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T10:46:37.737396Z",
     "iopub.status.busy": "2021-12-07T10:46:37.73699Z",
     "iopub.status.idle": "2021-12-07T10:46:41.779343Z",
     "shell.execute_reply": "2021-12-07T10:46:41.778433Z",
     "shell.execute_reply.started": "2021-12-07T10:46:37.737358Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "creating a object for resnet50 and setting its input channel to 1 (for grayscale image)\n",
    "\"\"\"\n",
    "resnet_pretrained = models.resnet50(pretrained=True)\n",
    "resnet_pretrained.conv1 = nn.Conv2d(in_channels=1,out_channels=64,kernel_size=7,stride=2,padding=3,bias = False)\n",
    "\n",
    "# resnet_init = models.resnet50(pretrained=False)\n",
    "# resnet_init.conv1 = nn.Conv2d(in_channels=1,out_channels=64,kernel_size=7,stride=2,padding=3,bias =False)\n",
    "\n",
    "\"\"\"\n",
    "creating object for U-net with resnet50 encoder (Imagenet)\n",
    "\"\"\"\n",
    "resnet_preTrained_UNet = UNetWithResnet50Encoder(resnet_pretrained).to(device)\n",
    "# resnet_init_UNet = UNetWithResnet50Encoder(resnet_init).to(device)\n",
    "\n",
    "# attention_init_UNet = UNetWithAttentionEncoder().to(device)\n",
    "\"\"\"\n",
    "creating an object for U-net with resnet50 encoder BYOL pre-trained (Our method)\n",
    "\"\"\"\n",
    "new_resnet_byol_unet = UNetByolWithResnet50Encoder(new_encoder_Resnet).to(device)\n",
    "\n",
    "\"\"\"\n",
    "creating an object for U-net with resnet50 encoder BYOL pre-trained (Original)\n",
    "\"\"\"\n",
    "# original_resnet_byol_unet = UNetByolWithResnet50Encoder(old_encoder_Resnet).to(device)\n",
    "# new_resnet_byol_unet_color = UNetByolWithResnet50EncoderColor(new_encoder_Resnet).to(device)\n",
    "\n",
    "\"\"\"\n",
    "creating an object for U-net with self-attention encoder BYOL pre-trained (our method)\n",
    "\"\"\"\n",
    "new_attention_byol_unet = UNetByolWithAttentionEncoder(new_encoder_attention).to(device)\n",
    "\n",
    "\"\"\"creating an object for U-net with self-attention encoder BYOL pre-trained (our method)\"\"\"\n",
    "original_attention_byol_unet = UNetByolWithAttentionEncoder(attention_byol_original_encoder).to(device)\n",
    "\n",
    "# old_resnet_byol = UNetByolWithResnet50Encoder(old_encoder_Resnet).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T10:46:41.781104Z",
     "iopub.status.busy": "2021-12-07T10:46:41.780765Z",
     "iopub.status.idle": "2021-12-07T10:46:41.810872Z",
     "shell.execute_reply": "2021-12-07T10:46:41.810141Z",
     "shell.execute_reply.started": "2021-12-07T10:46:41.781055Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "changing the value of input_channel to 1 or 3 as per the input image.\n",
    "\"\"\"\n",
    "\n",
    "new_resnet_byol_unet.input_block[0] = nn.Conv2d(in_channels=3,out_channels=64,kernel_size=7,stride=2,padding=3,bias=False)\n",
    "new_resnet_byol_unet = new_resnet_byol_unet.to(device)\n",
    "\n",
    "new_attention_byol_unet.input_block[0] = nn.Conv2d(in_channels=3,out_channels=64,kernel_size=7,stride=2,padding=3,bias=False)\n",
    "new_attention_byol_unet = new_attention_byol_unet.to(device)\n",
    "\n",
    "# old_resnet_byol.input_block[0] = nn.Conv2d(in_channels=1,out_channels=64,kernel_size=7,stride=2,padding=3,bias=False)\n",
    "# old_resnet_byol = old_resnet_byol.to(device)\n",
    "\n",
    "original_attention_byol_unet.input_block[0] = nn.Conv2d(in_channels=1,out_channels=64,kernel_size=7,stride=2,padding=3,bias=False)\n",
    "original_attention_byol_unet = original_attention_byol_unet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T10:46:41.813827Z",
     "iopub.status.busy": "2021-12-07T10:46:41.813251Z",
     "iopub.status.idle": "2021-12-07T10:46:41.836482Z",
     "shell.execute_reply": "2021-12-07T10:46:41.835699Z",
     "shell.execute_reply.started": "2021-12-07T10:46:41.813787Z"
    }
   },
   "outputs": [],
   "source": [
    "ALPHA = 0.8\n",
    "GAMMA = 2\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, alpha=ALPHA, gamma=GAMMA, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        #first compute binary cross-entropy \n",
    "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "        BCE_EXP = torch.exp(-BCE)\n",
    "        focal_loss = alpha * (1-BCE_EXP)**gamma * BCE\n",
    "                       \n",
    "        return focal_loss\n",
    "\n",
    "\n",
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "#         inputs = torch.sigmoid(inputs)\n",
    "\n",
    "        bce_weight = 0.5\n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "        loss_final = BCE * bce_weight + dice_loss * (1 - bce_weight)\n",
    "        return loss_final\n",
    "    \n",
    "## IOU computation\n",
    "def iou_(y_pred,y):\n",
    "    inputs = y_pred.reshape(-1)\n",
    "    targets = y.reshape(-1)\n",
    "    intersection = (inputs * targets).sum()\n",
    "    total = (inputs + targets).sum()\n",
    "    union = total - intersection \n",
    "    smooth = 1    \n",
    "    iou = (intersection + smooth)/(union + smooth)\n",
    "    return iou\n",
    "\n",
    "\n",
    "\n",
    "def iou_batch(y_pred,y):\n",
    "    '''computes mean iou for a batch of ground truth masks and predicted masks'''\n",
    "    ious = []\n",
    "#     y_pred = torch.sigmoid(y_pred)\n",
    "    y_pred = y_pred.clone().cpu().detach().numpy()\n",
    "    y = y.clone().cpu().detach().numpy() \n",
    "    \n",
    "    for pred, label in zip(y_pred, y):\n",
    "        ious.append(iou_(pred, label))\n",
    "    iou = np.nanmean(ious)\n",
    "    return iou \n",
    "\n",
    "\n",
    "# def mIoU(pred_mask, mask, smooth=1e-10, n_classes=2):\n",
    "#     with torch.no_grad():\n",
    "#         pred_mask = F.softmax(pred_mask, dim=1)\n",
    "        \n",
    "#         pred_mask = torch.argmax(pred_mask, dim=1)\n",
    "#         pred_mask = pred_mask.contiguous().view(-1)\n",
    "#         mask = mask.contiguous().view(-1)\n",
    " \n",
    "#         # print(pred_mask.shape)\n",
    "#         # print(mask.shape)\n",
    "#         iou_per_class = []\n",
    "#         for clas in range(0, n_classes): #loop per pixel class\n",
    "#             true_class = pred_mask == clas\n",
    "#             true_label = mask == clas\n",
    "\n",
    "#             if true_label.long().sum().item() == 0: #no exist label in this loop\n",
    "#                 iou_per_class.append(np.nan)\n",
    "#             else:\n",
    "#                 intersect = torch.logical_and(true_class, true_label).sum().float().item()\n",
    "#                 union = torch.logical_or(true_class, true_label).sum().float().item()\n",
    "\n",
    "#                 iou = (intersect + smooth) / (union +smooth)\n",
    "#                 iou_per_class.append(iou)\n",
    "#         print(iou_per_class)\n",
    "#         return np.nanmean(iou_per_class)\n",
    "    \n",
    "# def jaccard_index_numpy(y_true, y_pred):\n",
    "#     \"\"\"Define Jaccard index.\n",
    "#        Parameters\n",
    "#        ----------\n",
    "#        y_true : N dim Numpy array\n",
    "#            Ground truth masks. E.g. ``(num_of_images, x, y, channels)`` for 2D \n",
    "#            images or ``(volume_number, z, x, y, channels)`` for 3D volumes.\n",
    "#        y_pred : N dim Numpy array\n",
    "#            Predicted masks. E.g. ``(num_of_images, x, y, channels)`` for 2D \n",
    "#            images or ``(volume_number, z, x, y, channels)`` for 3D volumes.\n",
    "#        Returns\n",
    "#        -------\n",
    "#        jac : float\n",
    "#            Jaccard index value.\n",
    "#     \"\"\"\n",
    "\n",
    "#     TP = np.count_nonzero(y_pred * y_true)\n",
    "#     FP = np.count_nonzero(y_pred * (y_true - 1))\n",
    "#     FN = np.count_nonzero((y_pred - 1) * y_true)\n",
    "\n",
    "#     if (TP + FP + FN) == 0:\n",
    "#         jac = 0\n",
    "#     else:\n",
    "#         jac = TP / (TP + FP + FN)\n",
    "\n",
    "#     return jac\n",
    "\n",
    "\n",
    "# def jaccard_index_numpy_without_background(y_true, y_pred):\n",
    "#     \"\"\"Define Jaccard index excluding the background class (first channel). \n",
    "#        Parameters\n",
    "#        ----------\n",
    "#        y_true : N dim Numpy array\n",
    "#            Ground truth masks. E.g. ``(num_of_images, x, y, channels)`` for 2D\n",
    "#            images or ``(volume_number, z, x, y, channels)`` for 3D volumes.\n",
    "#        y_pred : N dim Numpy array\n",
    "#            Predicted masks. E.g. ``(num_of_images, x, y, channels)`` for 2D\n",
    "#            images or ``(volume_number, z, x, y, channels)`` for 3D volumes.\n",
    "#        Returns\n",
    "#        -------\n",
    "#        jac : float\n",
    "#            Jaccard index value.\n",
    "#     \"\"\"\n",
    "\n",
    "#     TP = np.count_nonzero(y_pred[...,1:] * y_true[...,1:])\n",
    "#     FP = np.count_nonzero(y_pred[...,1:] * (y_true[...,1:] - 1))\n",
    "#     FN = np.count_nonzero((y_pred[...,1:] - 1) * y_true[...,1:])\n",
    "\n",
    "#     if (TP + FP + FN) == 0:\n",
    "#         jac = 0\n",
    "#     else:\n",
    "#         jac = TP / (TP + FP + FN)\n",
    "\n",
    "#     return jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T10:46:41.83839Z",
     "iopub.status.busy": "2021-12-07T10:46:41.837947Z",
     "iopub.status.idle": "2021-12-07T10:46:41.848725Z",
     "shell.execute_reply": "2021-12-07T10:46:41.847914Z",
     "shell.execute_reply.started": "2021-12-07T10:46:41.83835Z"
    }
   },
   "outputs": [],
   "source": [
    "#ref https://towardsdatascience.com/how-to-save-and-load-a-model-in-pytorch-with-a-complete-example-c2920e617dee\n",
    "\n",
    "def save_ckp(state, is_best, checkpoint_path, best_model_path):\n",
    "    \"\"\"\n",
    "    state: checkpoint we want to save\n",
    "    is_best: is this the best checkpoint; min validation loss\n",
    "    checkpoint_path: path to save checkpoint\n",
    "    best_model_path: path to save best model\n",
    "    \"\"\"\n",
    "    # save checkpoint data to the path given, checkpoint_path\n",
    "    torch.save(state, checkpoint_path)\n",
    "    # if it is a best model, min validation loss\n",
    "    if is_best:\n",
    "        # copy that checkpoint file to best path given, best_model_path\n",
    "        shutil.copyfile(checkpoint_path, best_model_path)\n",
    "        \n",
    "def load_ckp(checkpoint_fpath, model, optimizer):\n",
    "    \"\"\"\n",
    "    checkpoint_path: path to save checkpoint\n",
    "    model: model that we want to load checkpoint parameters into       \n",
    "    optimizer: optimizer we defined in previous training\n",
    "    \"\"\"\n",
    "    # load check point\n",
    "    checkpoint = torch.load(checkpoint_fpath)\n",
    "    # initialize state_dict from checkpoint to model\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    # initialize optimizer from checkpoint to optimizer\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
    "    valid_loss_min = checkpoint['valid_loss_min']\n",
    "    # return model, optimizer, epoch value, min validation loss \n",
    "    return model, optimizer, checkpoint['epoch'], valid_loss_min.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T10:46:41.850525Z",
     "iopub.status.busy": "2021-12-07T10:46:41.850129Z",
     "iopub.status.idle": "2021-12-07T16:51:09.761317Z",
     "shell.execute_reply": "2021-12-07T16:51:09.758406Z",
     "shell.execute_reply.started": "2021-12-07T10:46:41.850492Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_path = '/kaggle/working/chkpoint_'\n",
    "best_model_path = '/kaggle/working/bestmodel.pt'\n",
    "# new_resnet_byol_unet,_,_,_ = load_ckp(best_model_path, new_resnet_byol_unet,optimizer)\n",
    "epochs = 50\n",
    "criterion = DiceBCELoss()\n",
    "#criterion = FocalLoss()\n",
    "# criterion = nn.BCELoss()\n",
    "learning_rate = 0.003\n",
    "\n",
    "#set Adam optimizer and change the name of the unet as required\n",
    "optimizer = torch.optim.Adam(new_attention_byol_unet.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.RMSprop(new_attention_byol_unet.parameters(), lr=0.0005, alpha=0.99, eps=1e-08, weight_decay=0.0003, momentum=0.99, centered=False)\n",
    "# optimizer = torch.optim.SGD(new_attention_byol_unet.parameters(),momentum=0.99,lr = learning_rate)\n",
    "\n",
    "#set the scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience = 15, verbose = 1)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer,base_lr=0.000007,max_lr=0.05,step_size_up=5,mode='triangular',cycle_momentum=False)\n",
    "valid_loss_min = 3.95275\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "training of the model\n",
    "\"\"\"\n",
    "train_loss,val_loss = [],[]\n",
    "train_iou,val_iou = [],[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch {}/{}'.format(epoch + 1, epochs))\n",
    "    start_time = time.time()\n",
    "    \n",
    "    running_train_loss = []\n",
    "    running_train_score = []\n",
    "    \n",
    "    #change the name of the unet as required\n",
    "    new_attention_byol_unet.train()\n",
    "    \n",
    "    #change the name of the dataloader as required\n",
    "    for image,mask in train_loader_segmentation_luchi:\n",
    "            image = image.to(device,dtype=torch.float)\n",
    "            mask = mask.to(device,dtype=torch.float)\n",
    "#             print(image.shape)\n",
    "\n",
    "            #change the name of the unet as required\n",
    "            pred_mask = new_attention_byol_unet.forward(image) \n",
    "#             print(pred_mask.shape)\n",
    "#             print(mask.shape)\n",
    "            # forward propogation\n",
    "            loss = criterion(pred_mask,mask)\n",
    "            score = iou_batch(pred_mask,mask)\n",
    "            optimizer.zero_grad() # setting gradient to zero\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss.append(loss.item())\n",
    "            running_train_score.append(score.item())\n",
    "#     lr_scheduler.step(np.mean(running_train_loss))   \n",
    "#     lr_scheduler.step()\n",
    "\n",
    "              \n",
    "    running_val_loss = []\n",
    "    running_val_score = []\n",
    "    \n",
    "    \"\"\"\n",
    "    Validation part \n",
    "    \"\"\"\n",
    "    \n",
    "    #change the name of the unet as required\n",
    "    new_attention_byol_unet.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for image,mask in val_loader_segmentation_luchi:\n",
    "            image = image.to(device,dtype=torch.float)\n",
    "            mask = mask.to(device,dtype=torch.float)\n",
    "            \n",
    "            #change the name of the unet as required\n",
    "            pred_mask = new_attention_byol_unet.forward(image)\n",
    "            loss = criterion(pred_mask,mask)\n",
    "            score = iou_batch(pred_mask,mask)\n",
    "            running_val_loss.append(loss.item())\n",
    "            running_val_score.append(score.item())\n",
    "    lr_scheduler.step(np.mean(running_val_loss)) \n",
    "\n",
    "    epoch_train_loss,epoch_train_score = np.mean(running_train_loss) ,np.mean(running_train_score)\n",
    "    print('Train loss : {} iou : {}'.format(epoch_train_loss,epoch_train_score))                       \n",
    "    train_loss.append(epoch_train_loss)\n",
    "    train_iou.append(epoch_train_score)\n",
    "    \n",
    "    epoch_val_loss,epoch_val_score = np.mean(running_val_loss),np.mean(running_val_score)\n",
    "    print('Validation loss : {} iou : {}'.format(epoch_val_loss,epoch_val_score))                                \n",
    "    val_loss.append(epoch_val_loss)\n",
    "    val_iou.append(epoch_val_score)\n",
    "   \n",
    "\n",
    "\"\"\"\n",
    "creating checkpoint and saving model\n",
    "\"\"\"\n",
    "#     create checkpoint variable and add important data\n",
    "    checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'valid_loss_min': epoch_val_loss,\n",
    "            'state_dict': new_attention_byol_unet.state_dict(),  #change the name of the unet as required\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }\n",
    "    \n",
    "    # save checkpoint\n",
    "    save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n",
    "    ## TODO: save the model if validation loss has decreased\n",
    "    if epoch_val_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n",
    "            # save checkpoint as best model\n",
    "            save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n",
    "            valid_loss_min = epoch_val_loss\n",
    "    time_elapsed = time.time() - start_time\n",
    "    print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T16:51:09.771148Z",
     "iopub.status.busy": "2021-12-07T16:51:09.770768Z",
     "iopub.status.idle": "2021-12-07T16:51:09.806228Z",
     "shell.execute_reply": "2021-12-07T16:51:09.805405Z",
     "shell.execute_reply.started": "2021-12-07T16:51:09.771111Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n",
    "                   interpolation=None):\n",
    "    \"\"\"Display the given set of images, optionally with titles.\n",
    "    images: list or array of image tensors in HWC format.\n",
    "    titles: optional. A list of titles to display with each image.\n",
    "    cols: number of images per row\n",
    "    cmap: Optional. Color map to use. For example, \"Blues\".\n",
    "    norm: Optional. A Normalize instance to map values to colors.\n",
    "    interpolation: Optional. Image interporlation to use for display.\n",
    "    \"\"\"\n",
    "    titles = titles if titles is not None else [\"\"] * len(images)\n",
    "    rows = len(images) // cols + 1\n",
    "    plt.figure(figsize=(14, 14 * rows // cols))\n",
    "    i = 1\n",
    "    for image, title in zip(images, titles):\n",
    "        plt.subplot(rows, cols, i)\n",
    "        plt.title(title, fontsize=9)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(image.astype(np.uint8), cmap=cmap,\n",
    "                   norm=norm, interpolation=interpolation)\n",
    "        i += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T16:51:09.807857Z",
     "iopub.status.busy": "2021-12-07T16:51:09.807516Z",
     "iopub.status.idle": "2021-12-07T16:51:09.835815Z",
     "shell.execute_reply": "2021-12-07T16:51:09.834873Z",
     "shell.execute_reply.started": "2021-12-07T16:51:09.80782Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_img(image,mask,prediction):\n",
    "\n",
    "  plt.figure(figsize=(20,10))  \n",
    "  img = image[0].squeeze()\n",
    "  msk = mask[0].squeeze()\n",
    "  pdt_msk = prediction[0].squeeze()\n",
    "\n",
    "  \n",
    "  pyplot.subplot(1, 3, 1) # second plot\n",
    "  pyplot.imshow(img.detach().cpu())\n",
    "\n",
    "  pyplot.subplot(1,3,2)\n",
    "  pyplot.imshow(msk.detach().cpu(),cmap = 'gray')\n",
    "\n",
    "  pyplot.subplot(1, 3, 3) # second plot\n",
    "  pyplot.imshow(pdt_msk.detach().cpu(),cmap ='gray')\n",
    "\n",
    "  \n",
    "  # msk = mask[1].squeeze()\n",
    "  # pdt_msk = prediction[1].squeeze()\n",
    "\n",
    "  # pyplot.subplot(2,2,1)\n",
    "  # pyplot.imshow(msk.detach().cpu())\n",
    "\n",
    "  # pyplot.subplot(2, 2, 2) # second plot\n",
    "  # pyplot.imshow(pdt_msk.detach().cpu())\n",
    "\n",
    "  # msk = mask[2].squeeze()\n",
    "  # pdt_msk = prediction[2].squeeze()\n",
    "\n",
    "  # pyplot.subplot(3,2,1)\n",
    "  # pyplot.imshow(msk.detach().cpu())\n",
    "\n",
    "  # pyplot.subplot(3, 2, 2) # second plot\n",
    "  # pyplot.imshow(pdt_msk.detach().cpu())\n",
    "\n",
    "  # msk = mask[3].squeeze()\n",
    "  # pdt_msk = prediction[3].squeeze()\n",
    "\n",
    "  # pyplot.subplot(4,2,1)\n",
    "  # pyplot.imshow(msk.detach().cpu())\n",
    "\n",
    "  # pyplot.subplot(4, 2, 2) # second plot\n",
    "  # pyplot.imshow(pdt_msk.detach().cpu())\n",
    "\n",
    "  # msk = mask[4].squeeze()\n",
    "  # pdt_msk = prediction[4].squeeze()\n",
    "\n",
    "  # pyplot.subplot(5,2,1)\n",
    "  # pyplot.imshow(msk.detach().cpu())\n",
    "\n",
    "  # pyplot.subplot(5, 2, 2) # second plot\n",
    "  # pyplot.imshow(pdt_msk.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T16:51:09.949838Z",
     "iopub.status.busy": "2021-12-07T16:51:09.948597Z",
     "iopub.status.idle": "2021-12-07T16:51:09.962444Z",
     "shell.execute_reply": "2021-12-07T16:51:09.961412Z",
     "shell.execute_reply.started": "2021-12-07T16:51:09.949806Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to test the model\n",
    "\"\"\"\n",
    "\n",
    "from matplotlib import pyplot\n",
    "def test_model(model,dataset,criterion):\n",
    "\n",
    "  running_test_loss = []\n",
    "  running_test_score = []\n",
    "  model.eval()  \n",
    "  with torch.no_grad():\n",
    "    for image,mask in dataset:\n",
    "        image = image.to(device,dtype = torch.float)\n",
    "        mask = mask.to(device,dtype = torch.float)   \n",
    "        predicted_mask = model.forward(image)\n",
    "        loss = criterion(predicted_mask,mask)\n",
    "        score = iou_batch(predicted_mask,mask)\n",
    "#         plot_img(image,mask,prediction=predicted_mask)\n",
    "        running_test_loss.append(loss.item())\n",
    "        running_test_score.append(score)\n",
    "    return np.mean(running_test_loss),np.mean(running_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T16:51:09.964224Z",
     "iopub.status.busy": "2021-12-07T16:51:09.963716Z",
     "iopub.status.idle": "2021-12-07T16:53:28.426714Z",
     "shell.execute_reply": "2021-12-07T16:53:28.425812Z",
     "shell.execute_reply.started": "2021-12-07T16:51:09.96419Z"
    }
   },
   "outputs": [],
   "source": [
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# swa_model = swa_model.cuda()\n",
    "\"\"\"\n",
    "loading the best performing weights and calling the test function to display the results\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#change the name of the unet as required\n",
    "new_attention_byol_unet,_,_,_ = load_ckp(best_model_path, new_attention_byol_unet,optimizer)\n",
    "\n",
    "#change the name of the unet as required\n",
    "loss , score = test_model(new_attention_byol_unet,test_loader_segmentation_luchi,criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T16:53:28.428304Z",
     "iopub.status.busy": "2021-12-07T16:53:28.427961Z",
     "iopub.status.idle": "2021-12-07T16:53:28.436813Z",
     "shell.execute_reply": "2021-12-07T16:53:28.434317Z",
     "shell.execute_reply.started": "2021-12-07T16:53:28.42827Z"
    }
   },
   "outputs": [],
   "source": [
    "print(loss)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Dataset_for_test(Dataset):\n",
    "#   def __init__(self,df,transform = None):\n",
    "#     self.df = df\n",
    "#     self.transforms = transform\n",
    "\n",
    "#   def __len__(self):\n",
    "#     return len(self.df)\n",
    "\n",
    "#   def __getitem__(self,idx):\n",
    "#     img_path = self.df['img'][idx]\n",
    "#     mask_path = self.df['mask'][idx]\n",
    "\n",
    "#     X = cv2.imread(img_path,cv2.IMREAD_GRAYSCALE)\n",
    "#     X = np.expand_dims(X,axis=-1)\n",
    "\n",
    "#     Y = cv2.imread(mask_path,cv2.IMREAD_GRAYSCALE)\n",
    "#     Y = np.expand_dims(Y,axis=-1)\n",
    "\n",
    "#     mask = np.zeros((Y.shape[0],Y.shape[1],1))\n",
    "#     mask = np.maximum(mask,Y)\n",
    "#     mask = mask.astype(\"float32\")\n",
    "    \n",
    "# #     if transform:\n",
    "# #         transformed = self.transforms(image=X, mask = mask)\n",
    "# #         img = transformed['image']\n",
    "# #         mask = transformed['mask']\n",
    "    \n",
    "#     img = cv2.resize(X,(224,224),interpolation=cv2.INTER_NEAREST)\n",
    "#     mask = cv2.resize(mask,(224,224),interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "#     mask = mask/255\n",
    "#     img = img/255\n",
    "#     mask = np.expand_dims(mask,axis=-1).transpose((2,0,1))\n",
    "#     img = np.expand_dims(img,axis=0)\n",
    "\n",
    "#     img = torch.from_numpy(img)\n",
    "#     img.unsqueeze(0)\n",
    "#     mask = torch.from_numpy(mask)\n",
    "\n",
    "#     return (img,mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_test_luchi = Dataset_for_test(test_df)\n",
    "\n",
    "# # trainset_luchi, valset_luchi = random_split(data_train_luchi, [train_split_luchi,test_split_luchi])\n",
    "# # trainset_kasthuri,valset_kasthuri = random_split(data_train_kasthuri,[train_split_kasthuri,test_split_kasthuri])\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(dataset=data_test_luchi, batch_size=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for img,mask in train_loader:\n",
    "#     img = img.to(device,dtype = torch.float)\n",
    "#     mask = mask.to(device,dtype = torch.float)   \n",
    "#     predicted_mask = new_attention_byol_unet.forward(img)\n",
    "#     print(predicted_mask.shape)\n",
    "# #     loss = criterion(predicted_mask,mask)\n",
    "# #     score = iou_batch(predicted_mask,mask)\n",
    "#     plot_img(img,mask,prediction=predicted_mask)\n",
    "# #     running_test_loss.append(loss.item())\n",
    "# #     running_test_score.append(score)\n",
    "# # return np.mean(running_test_loss),np.mean(running_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "function to plot the traing and validation loss and iou\n",
    "\"\"\"\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_loss,label='train_loss')\n",
    "plt.plot(val_loss,label='val_loss')\n",
    "plt.legend()\n",
    "plt.title('Loss Plot')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(train_iou,label='train_iou')\n",
    "plt.plot(val_iou,label='val_iou')\n",
    "plt.legend()\n",
    "plt.title('IOU Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to plot the image and its corresponding mask\n",
    "\"\"\"\n",
    "\n",
    "iter_ = iter(val_loader_segmentation_luchi)\n",
    "image,mask = next(iter_)\n",
    "image = image.to(device,dtype=torch.float)\n",
    "mask = mask.to(device,dtype=torch.float)\n",
    "y_pred = new_attention_byol_unet.forward(image)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,15))\n",
    "for i in range(0,3):\n",
    "    plt.subplot(3,5,i+1)\n",
    "    plt.title('Actual image')\n",
    "    plt.imshow(image_convert(image[i]))\n",
    "for i in range(0,3):\n",
    "    plt.subplot(3,5,i+5+1)\n",
    "    plt.title('Actual mask')\n",
    "    plt.imshow(mask_convert(mask[i]),cmap='gray')\n",
    "    \n",
    "    print(np.unique(np.array(mask[i].cpu().squeeze()).flatten()))\n",
    "for i in range(0,3):\n",
    "    plt.subplot(3,5,i+10+1)\n",
    "    plt.title('Predicted mask')\n",
    "    plt.imshow(mask_convert(y_pred[i]),cmap='gray')\n",
    "    print(np.unique(np.array(y_pred[i].detach().cpu().squeeze()).flatten()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
